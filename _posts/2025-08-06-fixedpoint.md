---
title: Nonexpansiveness and Fixed-Point Operations
date: 2025-08-06 05:50 +0800
description: Nonexpansive operators and fixed-point methods for optimization.
categories: [optimization, convex-optimization]
tags: [fixed-point]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## Convergence of Fixed Point Iterations

> **Definition** (Nonexpansive, Contractive Operators) An operator $T$ is said to be nonexpansive when
>
> $$
\| Tx - Ty \| \le \| x-y \| \quad \forall x,y \in \dom\,{T}
> $$
>
> i.e., $T$ is 1-Lipschitz. A contractive operator $T$ is $L$-Lipschitz with $L < 1$.

> **Definition** (Averaged Operators) An operator $T$ is $\theta$-averaged if
>
> $$
T = (1-\theta)I + \theta S
> $$
>
> for a nonexpansive operator $S$. If $\theta = 1/2$, then $T$ is called firmly nonexpansive.

> **Definition** (Fixed Points) $x$ is a fixed point of $T$ if $x = Tx$. We denote the set of all fixed points of $T$ as
>
> $$
\mathrm{Fix}\,T = \{ x \, | \, x = Tx \} = (I-T)^{-1}(0)
> $$

If $T$ is nonempty, then $\mathrm{Fix}\,T$ is closed and convex. A fixed-point operation refers to the sequence generated by $x^{k+1} =Tx^{k}$ for some $x^0 \in \R$. Our goal is to find a suitable $T$ whose fixed points are the solution to a monotone inclusion problem.

For contractive operators, the fixed point operation converges due to the Banach fixed-point theorem. We want to relax the contractive condition into an averaged condition.

> **Theorem** Let $T : \R^n \to \R^n$ be a $\theta$-averaged operator with $\theta \in (0,1)$, and $\mathrm{Fix}\,T \ne \emptyset$. Then the following statements hold.
> - The fixed point operation with any $x^0$ converges to some fixed point $x^\star \in \mathrm{Fix}\,T$. 
> - $\mathrm{dist}(x^k, \mathrm{Fix}\,T)$, $\| x^{k+1}-x^k \|$, and $\| x^k - x^\star \|$ for any fixed point $x^\star$ are monotonically nonincreasing with $k$.
> - $\mathrm{dist}(x^k, \mathrm{Fix}\,T) \to 0$ and 
> 
> $$
\| x^{k+1} - x^k \|^2 \le \frac{\theta}{(k+1)(1-\theta)} \mathrm{dist}^2(x^0,\mathrm{Fix}\,T) = O(1/k)
> $$

When $T$ is nonexpansive but not averaged, then we can use $(1-\theta) I + \theta T$ with $\theta \in (0,1)$, which yields an identical fixed point set.

To prove this, we use a simple lemma.

> **Lemma** Assume nonnegative sequences $V^0,V^1,\dots$ and $S^0,S^1,\dots$ satisfy
>
> $$
V^{k+1} \le V^k - S^k
> $$
>
> Then, $V^k$ is monotonically nonincreasing and $S^k \to 0$.

_proof_. The proof is simple. First it is trivial that $V^k$ is nonincreasing due to $S^k > 0$. Also, since

$$
\sum^k_{i=0} S^i \le V^0 - V^{k+1} \le V^0
$$

taking $k \to \infty$ leads us to

$$
\sum^\infty_{i=0} S^i < \infty
$$

and therefore, $S^i \to 0$. 

Now we prove the actual theorem.

_proof_. First we note that the following identity holds.

$$
\| (1-\theta) x + \theta y \|^2 = (1-\theta) \| x \|^2 + \theta \| y \|^2 - \theta(1-\theta) \| x-y \|^2
$$

Then, take $T = (1-\theta)I + \theta S$, and thus $x^{k+1} = Tx^k = (1-\theta)x^k + \theta Sx^k$. Also note that for any fixed point $x^\star$, $S(x^\star) = x^\star$. Therefore,

$$
\begin{aligned}
\| x^{k+1} - x^\star \|^2 &= (1-\theta) \| x^k - x^\star \|^2 + \theta \| S(x^k) - S(x^\star) \|^2 - \theta(1-\theta) \| S(x^k) - x^k \|^2 \\
&\le (1-\theta) \| x^k - x^\star \|^2 + \theta \| x^k - x^\star \|^2 - \theta (1-\theta) \| S(x^k) - x^k \|^2 \\
&= \| x^k - x^\star \|^2 - \theta(1-\theta) \| S(x^k) - x^k \|^2
\end{aligned}
$$

Apply the Lemma from above. Then $ \| x^{k+1} - x^\star \|$ is nonincreasing. By minimizing both sides with respect to $x^\star \in \mathrm{Fix}\,T$,

$$
\mathrm{dist}(x^{k+1},\mathrm{Fix}\,T) \le \mathrm{dist}(x^k,\mathrm{Fix}\,T)
$$

Since $Tx^k - x^k = x^{k+1} - x^k$, and $T$ is nonexpansive,

$$
\| x^{k+1}-x^k \| = \| Tx^k-Tx^{k-1} \| \le \| x^k-x^{k-1} \|
$$


Finally sum both side of the inequality to get

$$
\| x^{k+1} - x^\star \|^2 \le \| x^0 - x^\star \|^2 - \frac{1-\theta}{\theta} \sum^k_{j=0} \| Tx^j - x^j \|^2
$$

Then, since $\| Tx^j - x^j \| = \| x^{j+1}-x^j \|^2$ is nonincreasing,

$$
(k+1) \| x^{k+1}-x^\star \|^2 \le \sum^k_{j=0} \| Tx^j - x^j \|^2 \le \frac{\theta}{1-\theta} \| x^0-x^\star \|^2  - \frac{\theta}{1-\theta} \| x^{k+1} - x^\star \|^2
$$

We drop the last term. 

To prove convergence, we follow these steps.
- Consider any $\tilde{x}^\star \in \mathrm{Fix}\,T$. Then $x^0,\dots$ lie within the compact set 
$$\{ x \, | \, \| x-x^\star \| \le \| x^0-x^\star \| \}$$
, thus the sequence has an accumulation point $x^\star$.
- The accumulation point $x^\star$ satisfies $T(x^\star) = x^\star$, as $T(x^k) - x^k \to 0$ and $T-I$ is continuous. Therefore, $x^\star \in \mathrm{Fix}\,T$.
- Then we can conclude that, for such an accumulation point, $ \| x^k - x^\star \|$ monotonically decreases to 0 and therefore the entire sequence converges to $x^\star$.
The proof is complete.

## Examples of FPI

We now state three examples of optimization algorithms that can be viewed as a fixed-point iteration.

#### Gradient Descent

For the minimization problem

$$
\mathop{\text{minimize}}_{x \in \R^n} \quad f(x)
$$

where $f$ is differentiable, take the operator $I - \alpha \nabla f$ for $\alpha >0$ Consider the FPI

$$
x^{k+1} = x^k - \alpha \nabla f(x^k)
$$

Assume $f$ is $L$-smooth. Then by cocoersivity

$$
\| (I - (2/L)\nabla f)x - (I - (2/L) \nabla f)y \|^2 \le \| x-y \|^2
$$

Therefore, if we take $\alpha \in (0,2/L)$, then the FPI converges as $\theta = \alpha L/2 < 1$. The rate of convergence, as shown in the theorem above, is $O(1/k)$. If $f$ is strongly convex, then the FPI is a contraction.

#### Forward Step Method

Consider

$$
\mathop{\text{find}}_{x \in \R^n} \quad 0 = F(x)
$$

where $F : \R^n \to \R^n$. We reformulate the problem into find the fixed point of operator $I - \alpha F$ for any $\alpha >0$. Then the FPI

$$
x^{k+1} = x^k - \alpha Fx^k
$$

is the forward step method. The FPI converges if $F$ is $\beta$-cocoersive and $\alpha \in (0,2\beta)$ and the fixed point set is nonempty. The operator is contractive for small $\alpha$ and strongly monotone / Lipschitz $F$.

#### Dual Ascent

For the primal problem with linear constraints, we have the dual problem

$$
\text{maximize} \quad -f^* (-A^\top u) - b^\top u
$$

and the Lagrangian $L = f(x) + \langle u, Ax-b \rangle$. Applying the gradient method on $g(u) = f^* (-A^\top u) + b^\top u$, the FPI becomes

$$
\begin{aligned}
x^{k+1} &= \argmin_x \, L(x,u^k) \\
u^{k+1} &= u^k + \alpha (Ax^{k+1}-b)
\end{aligned}
$$

If $f$ is $\mu$-strongly convex, then $\nabla g$ is 
$$
\sigma^2_{\text{max}}(A)/\mu
$$
-Lipschitz. Also, if $f$ is $\mu$-strongly convex, total duality holds, and $0 < \alpha < 2\mu/\sigma^2_{\text{max}}(A)$, then $x^k$ and $u^k$ converge to optimal values.

## Reference
[1] Ryu, Ernest K., and Wotao Yin. 2022. Large-Scale Convex Optimization. Cambridge, England: Cambridge University Press.