---
title: Markov Decision Processes, Optimal Value Functions
date: 2025-07-22 20:00:00 +0800
description: Fundamentals of RL - MDPs and optimal value functions.
categories: [reinforcement-learning]
tags: [rl]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\SS{\mathcal{S}}
    \def\PP{\mathcal{P}}
    \def\AA{\mathcal{A}}
$$

## Rough Definition of RL

In reinforcement learning, unlike supervised or unsupervised learning, the optimal choice is computed using rewards given on-line. For instance, take a game of Atari. The input is given as the game state, and the output is the 'best possible' action the algorithm can take. The environment is the game software, and the reward is the score that is shown on screen. Initially, the algorithm doesn't have any knowledge about the game rules nor the outputs. As it plays the game, it improves the actions it takes at a certain circumstance. 

Mathematically, an environment can be modelled as a **Markov decision process (MDP)**. MDPs consists of a Markov process, a reward system, and an action. The goal of RL is to calculate the best possible (the generates the most amount of value) **policy**, which maps an input state to an action output. It is important to define the **reward system** according to the given problem. In general, use dynamic programming (DP) when the MDP is known, and use SARSA/Q-Learning if the MDP is unknown.

## Markov Decision Process

> **Definition** (Markov process) A Markov process is a pair $$(\SS, \PP)$$ where $\SS$ defines a (finite) set of states, and $\PP : S^2 \to [0,1]$ defines a state _transition probability_.

A Markov chain is 'historyless', i.e.,

$$
\mathrm{Pr}\left[ S_{t+1} = s' | S_t = s, S_{t-1}, \dots, S_1 \right] = \mathrm{Pr}\left[ S_{t+1} = s' | S_t = s \right] = \mathcal{P}(s,s')
$$

where each $S_t$ is the random variable describing the state at time $t'$.

> **Definition** (Markov Reward Process) A Markov reward process (MRP) is a tuple $$(\SS, \PP, R, \gamma)$$ where $(\SS, \PP)$ is a Markov process, $R : \SS \to \mathbb{R}$ is a _reward_ function, and $\gamma \in [0,1]$ is the _discount factor_.

The reward function represents the expected intermediate reward at the next state. Or equivalently, is the the _intermediate reward_ of the current state $s$.

> **Definition** (State-Value function) Given an MRP $(\SS, \PP, R, \gamma)$, its _state-value_ function $v : \SS \to \mathbb{R}$ is defined as follows.
>
> $$
v(s) \equiv \expectation\left[  R(s) + \gamma R\left( N_1(s) \right) + \gamma^2 R\left( N_2(s) \right) + \cdots \right] = \expectation \left[ \sum^\infty_{k=0} \gamma^k R\left( N_k(s) \right) \right]
> $$
> 
> Where $N_k(s)$ is the random variable describing the state after $k$ steps from $s$, i.e.,
>
> $$
\mathrm{Pr} \left[ N_k(s) = s' \right] = \sum_{s \in \SS} \left( \PP(s,s_1)\cdot\PP(s_1,s_2) \cdots \PP(s_{k-1}, s') \right)
> $$

If the discount factor $\gamma$ is close to 0, then the state-value is determined without considering 'greater future rewards' as opposed to the current 'optimal choice', i.e., it values the near future more than the far future. 

We can compactly describe the state-value function in matrix form by defining 
$$
v \equiv \left( v(s_1), \dots, v(s_n) \right)^T
$$
, 
$$
R \equiv \left( R(s_1), \dots, R(s_n) \right)^T
$$
, and 
$$
\left[\PP\right]_{ij} = \PP(s_i, s_j)
$$
. Then, the state-value function becomes

$$
v = (I - \gamma \PP)^{-1}R
$$

Now we define the Markov decision process.

> **Definition** (Markov Decision Process) A Markov decision process (MDP) is a tuple $(\SS, \AA, \PP, R, \gamma)$ where $S$ is a (finite) set of states, $\AA$ is a (finite) set of _actions_, $\PP : \SS \times \AA \times \SS \to [0,1]$ is the state transition probability, $R : \SS \times \AA \to \mathbb{R}$ is a reward function, and $\gamma$ is the discount factor.

Similar to before, $R(s,a)$ is the _expected reward_ when taking an action $a$ at a certain state $s$.

> **Definition** (Policies) A policy of an MDP $(\SS, \AA, \PP, R, \gamma)$ is a probability distribution over actions given states, i.e., $\pi : \SS \times \AA \to [0,1]$. A non-random policy is deterministic, i.e., $\pi : \SS \to \AA$.

A policy completely (deterministically probabilistically) defines the state transitions of an MDP. Therefore, given an MDP $(\SS, \AA, \PP, R, \gamma)$ and a policy $\pi$, we can define a Markovian _state_ and _reward_ sequences as follows. First, the state sequence $S_1, \dots $ is a Markov process $(\SS, \PP^\pi)$ where

$$
\PP^\pi (s, s') = \sum_{a \in \AA} \pi(s,a) \cdot \PP(s, a, s')
$$

The state/reward sequence is a Markov reward process $(\SS, \PP^\pi, R^\pi, \gamma)$ where

$$
R^\pi(s) = \sum_{a \in \AA} \pi(s,a) \cdot R(s,a)
$$

Correspondingly, we can define the state- and action-value functions given an MDP and policy. The state-value function $v^\pi:\SS \to \mathrm{R}$ is

$$
v^\pi(s) = \expectation \left[ \sum^\infty_{k=0} \gamma^k R^\pi \left( N_k^\pi (s) \right) \right]
$$

and the action-value function $q^\pi : \SS \times \AA \to \mathrm{R}$ is

$$
q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \SS} \left( \PP(s,a,s') \cdot v^\pi(s') \right)
$$

The state-value function of a policy can be understood as the expected _total reward_ starting from $s$, and following $\pi$. The action-value function can be understood as the expected total reward starting from $s$, taking action $a$, and then following $\pi$. Then we can reformulate RL as finding the maximum possible state/action valued function.

## Optimal Value Function

> **Definition** (Optimal State/Action-Value Functions) Given an MDP $(\SS, \AA, \PP, R, \gamma)$ the optimal state/action-value function is defined respectively as
>
>$$
v^* (s) = \max\{ v^\pi(s) \, | \text{ policy } \pi \text{ of the MDP } \} \text{ for each } s \in \SS 
>$$
>
>$$
q^*(s,a) = \max\{ q^\pi(s,a) \, | \text{ policy } \pi \} \text{ for each } s \in \SS, a \in \AA
>$$

Given an MDP, our goal is to find a policy that makes value functions **optimal**.

> **Definition** (Optimal Policy) A policy $\pi^*$ of an MDP is said to be optimal if
>
>$$
v^{ \pi^* }(s) = v^* (s) \text{ for all } s \in \SS
>$$

Then the natural question is to ask, whether if such an optimal policy exists, and whether the optimal policy for the state-value function is also optimal for the action-value function. The answer turns out to be yes for both questions.

> **Theorem** For every MDP,
> - there exists an optimal policy $\pi^*$
> - every optimal policy $ \pi^* $ achieves the optimal action-value function, i.e., $q^{ \pi^* }(s,a) = q^* (s,a)$ for all $s \in \SS$, $a \in \AA$
> - $v^* (s) = \max_{a \in A} q^* (s,a)$

Another extremely powerful theorem holds for the optimal policy.

> **Theorem** For every MDP, there exists a deterministic optimal policy.

Therefore, if an optimal action-value function $q^* (s,a)$ is available, an optimal policy is easily obtained by setting the probability of $ \argmax_{a \in \AA} \, q^* (s,a) $ to 1, and if else zero. Hereinafter, we denote the policy as $\pi : \SS \to \AA$, and write $\pi(s) = a$. Then how can we find the optimal action-value function?

> **Theorem** (Bellman Optimality Equations) Given an MDP $(\SS, \AA, \PP, R, \gamma)$, the following holds.
>
> $$
v^* (s) = \max_{a \in \AA} \left(  R(s,a) + \gamma \sum_{s' \in \SS} \PP(s,a,s') \cdot v^* (s) \right)
> $$
>
> $$
q^* (s,a) = R(s,a) + \gamma \sum_{s' \in \SS} \PP(s,a,s') \cdot \left( \max_{a' \in \AA} q^*(s',a') \right)
> $$

_proof_. TODO

These equations are self-consistent and nonlinear. They have no closed form solution in general. Therefore, we implement iterative methods to find an approximate solution. Some methods that are often used are: dynamic programming (policy iteration, value iteration), Monte-Carlo learning, and temporal-difference control (SARSA, Q-learning).

## References
[1] Silver, David. 2015. Lectures on Reinforcement Learning
[2] Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Adaptive Computation and Machine Learning Series. Cambridge, MA: Bradford Books.