---
title: SVD As a Low-Rank Approximation
date: 2025-07-14 11:33:00 +0800
description: qwer
categories: [Math, Statistics]
math: true
---
## Singular Value Decomposition

Take a real-valued $m\times n$ matrix $A$. Then its singular value decomposition is the following decomposition.

$$
A = U\Sigma V^T
$$

where $U$ and $V$ are each $m\times m$ and $n \times n$ orthogonal matrices, i.e., $U^T U= I$ and $V^T V = I$, and $\Sigma_{ij} = \lambda_i \delta_{ij}$. $\lambda_{ij}$ are the *singular values* of $A$. The singular values are equal to the square roots of the eigenvalues of $A^TA$ ($AA^T$), with eigenvectors $V$ ($U$). 

Hereinafter, $m > n$ without loss of generality. Then, we can perform *thin SVD*, where we truncate the last $m-n$ rows of $\Sigma$, which results in truncating the last $m-n$ columns of $U$. Now, $U$ is an $m \times n$ left-orthogonal matrix, i.e., $U^T U = I_{n\times n}$.

## Thin SVD, Truncated SVD

We can take this process a step further by truncating the singular values, which results in both $U$ and $V$ being truncated. If we leave $r$ largest singular values, then $U$ and $V$ become $m \times r$ and $r \times n$ respectively. Now consider

$$
\tilde{A}=\sum^r_{i=1} u_i\lambda_i v_i^T
$$

where $u_i$ and $v_i$ is the $i$-th column of $U$ and $V$, respectively. This computation is called *truncated SVD*. The resulting matrix is a rank-$r$ matrix $\tilde{A}$.

**Theorem**. (Eckart-Young) $\tilde{A}$ is the best rank-$r$ approximation of $A$, in terms of both spectral norm (largest singular value) and Frobenius norm.

We can easily see that the spectral norm $||\tilde{A}-A||_2 = \lambda_{r+1}$ and Frobenius norm $||\tilde{A} - A||_F = \sqrt{\sum_{i=r+1}^N \lambda_i^2}$.
