---
title: Duality In Convex Optimization
date: 2025-08-03 05:50 +0800
description: Duality, the Lagrangian, Slater's Constraints, and saddle-point interpretation.
categories: [optimization, convex-optimization]
tags: [duality]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## The Lagrangian Dual Function

Consider an optimization problem in standard form, with domain $\mathcal{D}$ nonempty, and the optimal value $p^\star$

$$
\begin{aligned}
\text{minimize} \quad & f_0(x) \\
\text{subject to} \quad & f_i(x) < 0, \quad i = 1, \dots, m  \\
& h_i(x) = 0, \quad i = 1, \dots, p
\end{aligned}
$$

> **Definition** (Lagrangian) The Lagrangian $L : \R^n \times \R^m \times \R^p \to \R$ is
>
> $$
L(x,\lambda,\nu) = f_0(x) + \sum^m_{i=1} \lambda_i f_i(x) + \sum^p_{i=1} \nu_i h_i(x)
> $$
> 
> We call $\lambda_i$ and $\nu_i$ the Lagrange multipliers or the _dual variables_.

We then define the _dual function_.

> **Definition** (Dual Function) The dual function of a given optimization problem is
> 
> $$
g(\lambda, \nu) = \inf_{x \in \mathcal{D}} L(x,\lambda,\nu)
> $$

Since the dual function is the pointwise infimum of a family of affine functions of $(\lambda, \nu)$, it is concave.

An important property is that the dual function yields a lower bound on the optimal value $p^\star$ for any $\lambda \succeq 0$, which is easily verified from $f_i(x) < 0$ and $g_i(x) = 0$. For $\lambda \succeq 0$ and $(\lambda, \nu) \in \dom{g}$, we call this condition _dual feasible_ - and in such cases, the dual function yields a non-vacuous lower bound on $p^\star$. 

Therefore, by finding the maximum of $g$, we find the 'best lower bound' for $p^\star$. 

$$
\begin{aligned}
\text{maximize} \quad & g(\lambda, \nu) \\
\text{subject to} \quad & \lambda \succeq 0
\end{aligned}
$$

In most cases, the dual constraints are implicit and we can reformulate to state the constraints explicitly. For instance consider the standard LP problem.

$$
\begin{aligned}
\text{minimize} \quad & c^\top x \\
\text{subject to} \quad & Ax = b \\
& x \succeq 0
\end{aligned}
$$

Then the dual function is given by

$$
g(\lambda, \nu) = \begin{cases} -b^\top \nu & \text{if } A^\top v - \lambda + c = 0 \\ -\infty & \text{otherwise} \end{cases}
$$

Therefore, we can reformulate the dual problem into

$$
\begin{aligned}
\text{maximize} \quad & b^\top \nu \\
\text{subject to} \quad & A^\top \nu - \lambda + c = 0 \\
& \lambda \succeq 0
\end{aligned}
$$

## Weak and Strong Duality, Slater's Constraints

Denote the optimal value of the dual problem as $d^\star$, then because $g$ always yields a lower bound on $p^\star$, we automatically have

$$
d^\star \le p^\star
$$

We are interested if the equality, $d^\star = p^\star$ holds. In general, this is not true. However, if the primal problem is _convex_ in the sense of

$$
\begin{aligned}
\text{minimize} \quad & f_0(x) \\
\text{subject to} \quad & f_i(x) < 0, \quad i = 1, \dots, m  \\
& Ax = b
\end{aligned}
$$

for convex $f_i$, then with certain regularity conditions, the equality holds. This is called _strong duality_.

> **Theorem** (Slater's Conditions) If there exist $x \in \mathrm{ri}\,\mathcal{D}$ such that $f_i(x) < 0$ for $i = 1,\dots,m$, and $Ax = b$, then strong duality holds.

Suppose that strong duality holds, and denote $(\lambda^\star, \nu^\star)$ as the dual optimal point and $(x^\star)$ the primal optimal point. Then the following equation holds.

$$
\begin{aligned}
f_0(x^\star) &= g(\lambda^\star, \nu^\star) \\
             &= \inf_{x} \left( f_0(x) + \sum^m_{i=1} \lambda_i^\star f_i(x) + \sum^p_{i=1} \nu_i^\star h_i(x) \right) \\
             &\le f_0(x^\star) + \sum^m_{i=1} \lambda_i^\star f_i(x^\star) + \sum^p_{i=1} \nu_i^\star h_i(x^\star) \\
             &\le f_0(x^\star)
\end{aligned}
$$

The last inequality holds because $\lambda_i^\star \ge 0$, $f_i(x^\star) \le 0$ for $i=1,\dots,m$, and $h_i(x^\star)=0$. Since equality holds throughout, $\lambda_i^\star f_i(x^\star)=0$ and therefore if $\lambda_i^\star > 0$ then $f_i(x^\star) = 0$. This is called _complementary slackness_.

Now assume that all the functions are differentiable but make no assumptions about convexity. Then, since $x^\star$ minimizes $L(x,\lambda^\star, \nu^\star)$ over $x$, its gradient must vanish. Therefore,

$$
\nabla f_0(x^\star) + \sum^m_{i=1} \lambda_i^\star \nabla f_i(x^\star) + \sum^p_{i=1} \nu_i \nabla h_i(x^\star) = 0
$$

Enforcing this condition, along with the feasibility of primal and dual solutions, i.e., $f_i(x^\star) \le 0$, $h_i(x^\star) = 0$, $\lambda_i^\star \ge 0$, and complementary slackness, i.e., $\lambda_i^\star f_i(x^\star) = 0$ leads to the _Karush-Kuhn-Tucker_ (KKT) conditions - which, when strong duality holds, any pair of primal and dual optimal points must satisfy.

When the primal problem is convex, then the KKT conditions are also sufficient for the points to be primal and dual optimal. Therefore, if a convex optimization problem with differentiable objective and constraint functions satisfies Slater's condition, then the KKT conditions provide necessary and sufficient conditions for optimality.

## Saddle Function Interpretation

> **Definition** (Convex-concave) A function $L : \R^n \times \R^m \to \R \cup \{\pm \infty\}$ is convex-concave if $L$ is convex in $x$ when $u$ is fixed and concave in $u$ when $x$ is fixed.

$(x^\star, u^\star)$ is a saddle point of $L$ if

$$
L(x^\star, u) \le L(x^\star, u^\star) \le L(x^\star, u^\star) \quad \forall x \in \R^n, \, u \in \R^m
$$

Before, we formulated the primal-dual optimization problem with an asymmetrical view - the dual problem is generated from the primal problem. Instead of this view, we take the Lagrangian as a fundamental function and instead view the primal-dual problems symmetrically.

Therefore, from the Lagrangian $L(x,u)$, the primal problem generated is

$$
\mathop{\text{minimize}}_{x \in \R^n} \quad \sup_{u \in \R^m} L(x,u)
$$

and the dual problem becomes

$$
\mathop{\text{maximize}}_{u \in \R^m} \quad \inf_{x \in \R^n} L(x,u)
$$

For instance, take the Lagrangian $L(x,u) = f(x) + \langle u, Ax-b \rangle$. Such a Lagrangian generates a linearly constrained minimization primal-dual problem. The primal problem is

$$
\begin{aligned}
\mathop{\text{minimize}}_{x \in \R^n} \quad & f(x) \\
\text{subject to} \quad & Ax = b
\end{aligned}
$$

and dual problem

$$
\begin{aligned}
\mathop{\text{maximize}}_{u \in \R^m} \quad -f^* (-A^\top u) - b^\top u
\end{aligned}
$$

If the regularity condition 
$$\{x \, | \, Ax=b\} \cap \mathrm{int}\,\dom{f} \ne \emptyset$$
 holds, then strong duality holds and thus $d^\star = p^\star$. 

The Lagrangian $L(x,u) = f(x) + \langle u, Ax \rangle - g^* (u)$ generates the primal problem

$$
\begin{aligned}
\mathop{\text{minimize}}_{x \in \R^n} \quad f(x) + g(Ax)
\end{aligned}
$$

and dual problem

$$
\begin{aligned}
\mathop{\text{maximize}}_{u \in \R^m} \quad -f^* (-A^\top u) - g^* (u)
\end{aligned}
$$

which are called the _Fenchel-Rockafellar dual_. If 
$$A \, \dom {f} \cap \mathrm{int}\,\dom{g} \ne \emptyset$$
 holds, then strong duality holds and $d^\star = p^\star$.

From this interpretation, weak duality holds because of the max-min inequality. 

> **Theorem** (Max-min inequality) For any function $f : X \times U \to \R$,
>
> $$
\sup_{x \in X} \inf_{u \in U} f(x,u) \le \inf_{u \in U} \sup_{x \in X} f(x,u)
> $$

_proof_. For any $x,u$ we have

$$
\begin{gather*}
\inf_{x} f(x,u) \le f(x,u) \\
\sup_u \inf_x f(x,u) \le \sup_u f(x,u) \\
\sup_u \inf_x f(x,u) \le \inf_x \sup_u (x,u)
\end{gather*}
$$

where the third line holds, as the LHS is independent of $x$. Therefore, weak duality holds.

_Total duality_ refers to when both the primal and dual solutions exist, and strong duality holds. Then, the following theorem holds (equivalent to Slater's regularity conditions).

> **Theorem** Total duality holds if and only if $L$ has a saddle point. Moreover, the primal and dual optimal values are equal to the saddle-point function value, i.e., $p^\star = d^\star = L(\tilde{x}, \tilde{u})$.

_proof_. Assume $L$ has a saddle point $(x^\star, u^\star)$. Then

$$
\begin{aligned}
L(x^\star, u^\star) &= \inf_x L(x, u^\star) \\
                    &\le \sup_u \inf_x L(x,u) = d^\star \\
                    &\le \inf_x \sup_u L(x,u) = p^\star \\
                    &\le \sup_u L(x^\star,u) = L(x^\star, u^\star)
\end{aligned}
$$

And equality holds throughout. $\inf_x \sup_u L(x,u) = \sup_u L(x,u)$ so $x^\star$ is a primal solution, and $\inf_x L(x,u^\star) = \sup_u \inf_x L(x,u)$, so $u^\star$ is a dual solution. $d^\star = p^\star$ so strong duality holds.

Now assume total duality, and $x^\star, u^\star$ are primal, dual solutions. Then

$$
\begin{aligned}
\inf_x L(x,u^\star) &= \sup_u \inf_x L(x,u) = d^\star \\
                    &= \inf_x \sup_u L(x,u) = p^\star \\
                    &= \sup_u L(x^\star,u)
\end{aligned}
$$

Since $L(x^\star,u^\star) \le \sup_u L(x^\star,u) = \inf_x L(x,u^\star) \le L(x^\star,u^\star)$, we can conclude that $(x^\star, u^\star)$ is a saddle point. The proof is complete.

In this sense of viewing duality, we revisit the KKT conditions. For a convex optimization problem

$$
\begin{aligned}
\text{minimize} \quad & f_0(x) \\
\text{subject to} \quad & f_i(x) < 0, \quad i = 1, \dots, m  \\
& h_i(x) = 0, \quad i = 1, \dots, p
\end{aligned}
$$

where the functions $f_0, f_1, \dots, f_m$ are CCP and $h_1, \dots, h_p$ are affine. Then the Langrangian

$$
L(x,\lambda,\nu) = f_0(x) + \sum_i \lambda_i f_i(x) + \sum_i \nu_i h_i(x) - \delta_{\R^m_+}(\lambda)
$$

is convex-concave in $x$ and $(\lambda,\nu)$. We define the KKT operator

$$
\mathbb{T}(x,\lambda,\nu) = \begin{bmatrix} \partial_x L(x,\lambda,\nu) \\ \partial_\lambda(-L(x,\lambda,\nu)) \\ \partial_\nu(-L(x,\lambda,\nu)) \end{bmatrix} = \begin{bmatrix} \partial_x L(x,\lambda,\nu) \\ -F(x) + \mathbb{N}_{\R^m_+} (\lambda) \\ -H(x) \end{bmatrix}
$$

where $F = [f_1,\dots,f_m]^\top$ and $H = [h_1,\dots,h_p]^\top$. Then, from the KKT condition result above, $0 \in \mathbb{T}(x^\star, \lambda^\star, \nu^\star)$ if and only if total duality holds, and $x^\star, \lambda^\star, \nu^\star$ are the primal and dual optimal values. Therefore, we can think of solving the KKT conditions as finding the zero of the KKT operator.

## References
[1] Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge, England: Cambridge University Press.\
[2] Ryu, Ernest K., and Wotao Yin. 2022. Large-Scale Convex Optimization. Cambridge, England: Cambridge University Press.