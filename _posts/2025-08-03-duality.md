---
title: Duality In Convex Optimization
date: 2025-08-03 05:50 +0800
description: Duality, the Lagrangian, KKT conditions, and saddle-point interpretation.
categories: [optimization]
tags: [duality]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## The Lagrangian Dual Function

Consider an optimization problem in standard form, with domain $\mathcal{D}$ nonempty, and the optimal value $p^\star$

$$
\begin{aligned}
\text{minimize} \quad & f_0(x) \\
\text{subject to} \quad & f_i(x) < 0, \quad i = 1, \dots, m  \\
& h_i(x) = 0, \quad i = 1, \dots, p
\end{aligned}
$$

> **Definition** (Lagrangian) The Lagrangian $L : \R^n \times \R^m \times \R^p \to \R$ is
>
> $$
L(x,\lambda,\nu) = f_0(x) + \sum^m_{i=1} \lambda_i f_i(x) + \sum^p_{i=1} \nu_i h_i(x)
> $$
> 
> We call $\lambda_i$ and $\nu_i$ the Lagrange multipliers or the _dual variables_.

We then define the _dual function_.

> **Definition** (Dual Function) The dual function of a given optimization problem is
> 
> $$
g(\lambda, \nu) = \inf_{x \in \mathcal{D}} L(x,\lambda,\nu)
> $$

Since the dual function is the pointwise infimum of a family of affine functions of $(\lambda, \nu)$, it is concave.

An important property is that the dual function yields a lower bound on the optimal value $p^\star$ for any $\lambda \succeq 0$, which is easily verified from $f_i(x) < 0$ and $g_i(x) = 0$. For $\lambda \succeq 0$ and $(\lambda, \nu) \in \dom{g}$, we call this condition _dual feasible_ - and in such cases, the dual function yields a non-vacuous lower bound on $p^\star$. 

Therefore, by finding the maximum of $g$, we find the 'best lower bound' for $p^\star$. 

$$
\begin{aligned}
\text{maximize} \quad & g(\lambda, \nu) \\
\text{subject to} \quad & \lambda \succeq 0
\end{aligned}
$$

In most cases, the dual constraints are implicit and we can reformulate to state the constraints explicitly. For instance consider the standard LP problem.

$$
\begin{aligned}
\text{minimize} \quad & c^\top x \\
\text{subject to} \quad & Ax = b \\
& x \succeq 0
\end{aligned}
$$

Then the dual function is given by

$$
g(\lambda, \nu) = \begin{cases} -b^\top \nu & \text{if } A^\top v - \lambda + c = 0 \\ -\infty & \text{otherwise} \end{cases}
$$

Therefore, we can reformulate the dual problem into

$$
\begin{aligned}
\text{maximize} \quad & b^\top \nu \\
\text{subject to} \quad & A^\top \nu - \lambda + c = 0 \\
& \lambda \succeq 0
\end{aligned}
$$

## Weak and Strong Duality, Slater's Constraints

Denote the optimal value of the dual problem as $d^\star$, then because $g$ always yields a lower bound on $p^\star$, we automatically have

$$
d^\star \le p^\star
$$

We are interested if the equality, $d^\star = p^\star$ holds. In general, this is not true. However, if the primal problem is _convex_ in the sense of

$$
\begin{aligned}
\text{minimize} \quad & f_0(x) \\
\text{subject to} \quad & f_i(x) < 0, \quad i = 1, \dots, m  \\
& Ax = b
\end{aligned}
$$

for convex $f_i$, then with certain regularity conditions, the equality holds. This is called _strong duality_.

> **Theorem** (Slater's Conditions) If there exist $x \in \mathrm{ri}\,\mathcal{D}$ such that $f_i(x) < 0$ for $i = 1,\dots,m$, and $Ax = b$, then strong duality holds.

## Saddle Function Interpretation

> **Definition** (Convex-concave) A function $L : \R^n \times \R^m \to \R \cup \{\pm \infty\}$ is convex-concave if $L$ is convex in $x$ when $u$ is fixed and concave in $u$ when $x$ is fixed.

$(x^\star, u^\star)$ is a saddle point of $L$ if

$$
L(x^\star, u) \le L(x^\star, u^\star) \le L(x^\star, u^\star) \quad \forall x \in \R^n, \, u \in \R^m
$$

Before, we formulated the primal-dual optimization problem with an asymmetrical view - the dual problem is generated from the primal problem. Instead of this view, we take the Lagrangian as a fundamental function and instead view the primal-dual problems symmetrically.

Therefore, from the Lagrangian $L(x,u)$, the primal problem generated is

$$
\mathop{\text{minimize}}_{x \in \R^n} \quad \sup_{u \in \R^m} L(x,u)
$$

and the dual problem becomes

$$
\mathop{\text{maximize}}_{u \in \R^m} \quad \inf_{x \in \R^n} L(x,u)
$$

For instance, take the Lagrangian $L(x,u) = f(x) + \langle u, Ax-b \rangle$. Such a Lagrangian generates a linearly constrained minimization primal-dual problem. The primal problem is

$$
\begin{aligned}
\mathop{\text{minimize}}_{x \in \R^n} \quad & f(x) \\
\text{subject to} \quad & Ax = b
\end{aligned}
$$

and dual problem

$$
\begin{aligned}
\mathop{\text{maximize}}_{u \in \R^m} \quad -f^* (-A^\top u) - b^\top u
\end{aligned}
$$

If the regularity condition 
$$\{x \, | \, Ax=b\} \cap \mathrm{int}\,\dom{f} \ne \emptyset$$
 holds, then strong duality holds and thus $d^\star = p^\star$. 

The Lagrangian $L(x,u) = f(x) + \langle u, Ax \rangle - g^* (u)$ generates the primal problem

$$
\begin{aligned}
\mathop{\text{minimize}}_{x \in \R^n} \quad f(x) + g(Ax)
\end{aligned}
$$

and dual problem

$$
\begin{aligned}
\mathop{\text{maximize}}_{u \in \R^m} \quad -f^* (-A^\top u) - g^* (u)
\end{aligned}
$$

which are called the _Fenchel-Rockafellar dual_. If 
$$A \, \dom {f} \cap \mathrm{int}\,\dom{g} \ne \emptyset$$
 holds, then strong duality holds and $d^\star = p^\star$.

From this interpretation, weak duality holds because of the max-min inequality. 

> **Theorem** (Max-min inequality) For any function $f : X \times U \to \R$,
>
> $$
\sup_{x \in X} \inf_{u \in U} f(x,u) \le \inf_{u \in U} \sup_{x \in X} f(x,u)
> $$

_proof_. For any $x,u$ we have

$$
\begin{gather*}
\inf_{x} f(x,u) \le f(x,u) \\
\sup_u \inf_x f(x,u) \le \sup_u f(x,u) \\
\sup_u \inf_x f(x,u) \le \inf_x \sup_u (x,u)
\end{gather*}
$$

where the third line holds, as the LHS is independent of $x$. Therefore, weak duality holds.

_Total duality_ refers to when both the primal and dual solutions exist, and strong duality holds. Then, the following theorem holds (equivalent to Slater's regularity conditions).

> **Theorem** Total duality holds if and only if $L$ has a saddle point. Moreover, the primal and dual optimal values are equal to the saddle-point function value, i.e., $p^\star = d^\star = L(\tilde{x}, \tilde{u})$.

_proof_. Assume $L$ has a saddle point $(x^\star, u^\star)$. Then

$$
\begin{aligned}
L(x^\star, u^\star) &= \inf_x L(x, u^\star) \\
                    &\le \sup_u \inf_x L(x,u) = d^\star \\
                    &\le \inf_x \sup_u L(x,u) = p^\star \\
                    &\le \sup_u L(x^\star,u) = L(x^\star, u^\star)
\end{aligned}
$$

And equality holds throughout. $\inf_x \sup_u L(x,u) = \sup_u L(x,u)$ so $x^\star$ is a primal solution, and $\inf_x L(x,u^\star) = \sup_u \inf_x L(x,u)$, so $u^\star$ is a dual solution. $d^\star = p^\star$ so strong duality holds.

Now assume total duality, and $x^\star, u^\star$ are primal, dual solutions. Then

$$
\begin{aligned}
\inf_x L(x,u^\star) &= \sup_u \inf_x L(x,u) = d^\star \\
                    &= \inf_x \sup_u L(x,u) = p^\star \\
                    &= \sup_u L(x^\star,u)
\end{aligned}
$$

Since $L(x^\star,u^\star) \le \sup_u L(x^\star,u) = \inf_x L(x,u^\star) \le L(x^\star,u^\star)$, we can conclude that $(x^\star, u^\star)$ is a saddle point. The proof is complete.

## References
[1] Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge, England: Cambridge University Press.\
[2] Ryu, Ernest K., and Wotao Yin. 2022. Large-Scale Convex Optimization. Cambridge, England: Cambridge University Press.