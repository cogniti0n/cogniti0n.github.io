---
title: Regret Bounds & Adaptive Learning Rate Algorithms
date: 2025-09-05 05:50 +0800
description: Adaptive learning rate algorithms - AdaGrad, RMSProp, Adam, and modern variants
categories: [optimization, ml-optimization]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\R{\mathbb{R}}
$$

## Online Learning and Regret Bounds

In online learning, at each time step $t$, the algorithm picks a point $x_t \in \mathcal{F} \subseteq \R^d$. A loss function $f_t$ is then given, leading to a loss $f_t(x_t)$. The regret at the end of $T$ rounds is defined as

$$
R_T = \sum^T_{t=1} f_t(x_t) - \inf_{x \in \mathcal{F}} \sum^T_{t=1} f_t(x)
$$

The _greedy projection algorithm_ [1] is as follows. At time $t$, the learner recieves a subgradient $g_t \in \partial f_t(x_t)$. Then the next point is chosen as

$$
x_{t+1} = \Pi_{\mathcal{F}} (x_t - \eta_t g_t)
$$

For simplicity, we now consider unconstraind optimization. When $\eta_t = \eta / \sqrt{t}$, such an algorithm enjoys a regret bound of order $O(\sqrt{T})$.

## AdaGrad

AdaGrad is an _adaptive learning rate_ algorithm. It assigns a higher learning rate to infrequent features, leading to efficient optimization for sparse gradients. To do this, it considers previous subgradients, and adjusts the learning rates. Denote $G_t \equiv \sum^t_{\tau = 1} g_\tau g_\tau^\top $. Then AdaGrad is defined as

$$
x_{t+1} = x_t - \eta G_t^{-1/2} g_t
$$

Since this is computationally difficult to perform and also numerically unstable, an alternative method with heuristics is proposed.

$$
x_{t+1} = x_t - \eta \, \mathrm{diag}(\varepsilon I + G_t)^{-1/2} g_t
$$

Therefore, the update rules are 

$$
x_{t+1}^{(i)} = x_t^{(i)} - \eta \frac{g_t^{(i)}}{\sqrt{G_t^{(i,i)} + \varepsilon}}
$$

From this definition, it is clear that parameters associated with low-frequency features have larger learning rates, and therefore problems with sparse gradients converge quickly. AdaGrad has a regret bound of $O(\sqrt{T})$, which leads to a convergence rate of order $O(1/\sqrt{T})$.

Considering the dimension $d$ of the gradient vectors, online gradient descent yields best-case regret bound of $O(\sqrt{dT})$. In AdaGrad, for sparse cases, the regret bound is of order $O(\log{d}T)$, hence an exponentially better bound in the dimension $d$.

## RMSProp

AdaGrad suffers from two problems. First, it is sensitive to initial gradient size. Second, the accumulation of gradients lead to a learning rate near zero before arriving at a good local optima. Therefore, RMSProp uses a 'softer' update on the parameter $G_t$.

Instead of updating $G_{t} = G_{t-1} + g_t g_t^\top$, a 'decay rate' $\rho$ is used to give a weight to previous gradients.

$$
G_t = \rho G_{t-1} + (1-\rho) g_t g_t^\top
$$

An empirically good choice for $\rho$ is around $0.90$ ~ $0.95$. RMSProp works well in online and nonstationary settings.

## Adam

Adam is a _momentum based_ algorithm. At each time step $t$, the first and second moment estimate is updated using

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2
$$

where $g_t^2$ denotes an element-wise square operation. Then the coefficients for the updates are set as

$$
\alpha_t = \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}
$$

This is a _correction term_ set to take into account that $m_t$ and $v_t$ are biased towards 0 for the first few iterations. Finally, we achieve the parameter update rule

$$
x_{t} \gets x_{t-1} - \alpha_t \frac{m_t}{\sqrt{v_t} + \varepsilon}
$$

The original convergence proof for Adam included an error, as shown by [5]. Indeed, the authors of ref. [5] provided a convex optimization problem that could not be solved using Adam. However, it was shown in [6] that with constraints on the hyperparameters (specifically, as $\alpha \to 0$ and $\beta_2 \to 1$), Adam indeed converges with the same rate as AdaGrad.

## References

[1] Zinkevich, M. A. (2003). Online convex programming and generalized infinitesimal gradient ascent. International Conference on Machine Learning, 928–936. \
[2] Duchi, J. C., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research: JMLR, 12, 2121–2159. \
[3] Tieleman, T. and Hinton, G. (2012) Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude. COURSERA: Neural Networks for Machine Learning, 4, 26-31. \
[4] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1412.6980 \
[5] Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of Adam and beyond. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1904.09237 \
[6] Défossez, A., Bottou, L., Bach, F., & Usunier, N. (2020). A simple convergence proof of Adam and Adagrad. In arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2003.02395
