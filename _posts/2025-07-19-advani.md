---
title: Generalization Dynamics and Double-Descent Phenomenon In a Simple Neural Network
date: 2025-07-19 20:00:00 +0800
description: Part 1 of summary of Advani et al., Neural Networks 132, 2020, 428-46
categories: [machine-learning-deep-learning, deep-learning]
tags: [paper-reading]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\ww{\textbf{w}}
    \def\xx{\textbf{x}}
    \def\zz{\textbf{z}}
$$

## Overfitting Happens When Model Size is Close To Data Size

The authors first begin with a simple, linear teacher-student setup. The teacher network implements

$$
y^\mu = \ww \cdot \xx^\mu + \varepsilon^\mu
$$

where the superscript represents the data label (a total of $P$ data), and subscript represents the vector component. The weights and values are all vectors of dimension $N$. The values are sampled from

$$
\varepsilon^\mu \sim \mathcal{N}(0, \sigma_\varepsilon^2)
$$

$$
\ww_i \sim \mathcal{N}(0, \sigma_w^2)
$$

$$
\xx_i^\mu \sim \mathcal{N}(0,1/N)
$$

The student outputs $\hat{y}^\mu = \hat{\ww}\cdot \xx^\mu$. We optimize the loss function

$$
E_{\mathrm{tr}}=\frac{1}{P}\sum^P_{\mu=1} |y^\mu-\hat{y}^\mu|^2
$$

using gradient descent. For simplicity, denote $X \equiv [\xx^1 \, \cdots \, \xx^P]$, $y \equiv [y^1 \, \cdots, \, y^P]^T$, $\hat{y} = [\hat{y}^1 \, \cdots \, \hat{y}^P]^T$, and $\varepsilon \equiv [\varepsilon^1 \, \cdots \, \varepsilon^P]^T$ so that

$$
y=X^Tw+\varepsilon, \quad \hat{y}=X^T\hat{w}
$$

The continuous-time gradient descent dynamics are

$$
\frac{d\hat{\ww}}{dt}=-\gamma\frac{\partial E_{\mathrm{tr}}}{\partial \hat{\ww}}=\frac{2\gamma}{P}X[X^T(\ww-\hat{\ww})+\varepsilon]
$$

Peform eigendecomposition $\Sigma \equiv \frac{1}{P}XX^T = V \Lambda V^T$ where $V$ is orthogonal and $\Lambda$ is a diagonal matrix. Then automatically $X = \sqrt{P} V \Lambda^{1/2} U^T$ (SVD). Denote $\zz \equiv V^T \ww$ and $\hat{\zz} \equiv V^T \hat{\ww}$. Define $\tau^{-1} \equiv 2\gamma$. Rewriting the dynamics,

$$
\tau \frac{d\hat{\zz}}{dt}=\Lambda (\zz - \hat{\zz})+ \Lambda^{1/2}\frac{U^T\varepsilon}{\sqrt{P}}
$$

Then, $\tilde{\varepsilon} \equiv U^T\varepsilon /\sqrt{P}$ becomes another independent Gaussian variable with standard deviation $\sigma_\varepsilon$. By solving for each component of $\zz$, we obtain the full dynamics.

$$
z_i - \hat{z}_i(t) = [z_i-\hat{z}_i(0)] e^{-\lambda_i t / \tau} - \frac{\tilde{\varepsilon}_i}{\sqrt{\lambda_i}}(1-e^{-\lambda_i t /\tau}) \tag{1}
$$

Therefore, the generalization error $E_g = \left\langle (y-\hat{y})^2 \right\rangle _{y,\hat{y}}$ becomes (with some tedious calculations)

$$
E_g = \sigma_\varepsilon^2+\frac{1}{N}\sum^N_{i=1} \sigma_w^2 e^{-\frac{2\lambda_i t}{\tau}} + \frac{1}{N} \sum^N_{i=1} \left[\hat{\sigma}_0^2 e^{-\frac{2\lambda_i t}{\tau}}+\frac{\sigma_\varepsilon^2}{\lambda_i} \left( 1-e^{-\frac{\lambda_i t}{\tau}} \right)^2 \right]
$$

The first term is the squred bias term, and the second term is the variance. As time $t \to \infty$, the bias term decays to zero and the second term exponentially reaches the asymptote of $\sigma_\varepsilon^2 / \lambda_i$. This term corresponds to overfitting when noise is present.

There are two important factors to note.

- First, components with _exactly_ zero eigenvalues are 'frozen', i.e., no learning occurs. This does not hinder generalization, but performance becomes highly dependent on initial weights.

- Second, components with small-but-positive eigenvalues lead to catastrophic overfitting. See Eq. (1). When $\lambda_i \ll 1$, the noise parameter $\tilde{\varepsilon}_i$ has a larger impact on the weight dynamics. Therefore, the model overfits to random noise.

These results lead us to the conclusion that the gap between zero and the smallest positive eigenvalue determines generalization performance. We can theoretically analzye these results using random matrix theory by taking $N \to \infty$ while keeping $P/N=\alpha$ fixed. Then the sum over eigenvalues converges to

$$
\frac{1}{N}\sum^N_{i=1} \longrightarrow \int d\lambda \, \rho^{\mathrm{MP}}(\lambda)
$$

where $\rho^{\mathrm{MP}}$ is the _Marchenko-Pastur distribution_

$$
\rho^{\mathrm{MP}}(\lambda) = \begin{cases} \frac{1}{2\pi}\frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda},&\text{if }\lambda_-\le\lambda\le\lambda_+ \text{ or }\lambda=0 \\ 0, & \text{otherwise} \end{cases}
$$

where $\lambda_{\pm} = \left(\sqrt{\alpha}\pm1\right)^2$.

- When $\alpha < 1$, the number of parameters $N$ is greater than the number of data $P$, and thus the system is overparameterized. The model does not suffer from overfitting.
- When $\alpha > 1$, the system is underparametrized. The model also does not suffer from overfitting.
- When $\alpha = 1$, there is a clear peak of eigevalue distribution at $\lambda = 0$.

Correspondingly, catastrophic overfitting happens at $\alpha=1$. Indeed, if there are many degrees of freedom with $\lambda$ very close to zero, then the student cannot distinguish noise from signal.

Therefore, as the number of neurons reach $\infty$, a theoretical peak of generalization error is observed where $P \simeq N$. This phenomenon is called _double-descent_, where the model performs worst when the number of data is similar to the number of parameters, and increases in performance as the number of parameters highly exceeds the number of data.

---

## Ways to combat overfitting

### Early stopping

The generalization error reaches a maximum at a certain epoch, where the training is stopped. With the assumption $w(0) = 0$ (i.e., zero initial weights) The optimal stopping time is

$$
t^{opt}=\frac{\tau}{\lambda}\log(\mathrm{SNR}\cdot\lambda+1)
$$

When the variance of initial weights are small, then this theoretical value matches well with the experimental results.

### L2-regularization

Add a regularization term to the loss function.

$$
E_{\mathrm{tr}} = \frac{1}{P} \sum^P_{\mu=1} |y^\mu - \hat{y}^\mu|^2 + \Gamma \sum^N_{i=1} \hat{w}_i^2 = \frac{1}{P}\left|\mathbf{X}^T(\mathbf{w}-\mathbf{\hat{w}}) + \mathbf{\epsilon}\right|^2 + \Gamma |\mathbf{\hat{w}}|^2
$$

The resulting gradient descent dynamics become

$$
\tau \frac{d\hat{z}_i}{dt} = \lambda_i(z_i - \hat{z}_i) + \sqrt{\lambda_i} \tilde{\varepsilon}_i -\Gamma \hat{z}_i
$$

Therefore, the integrated dynamics are

$$
z_i - \hat{z}_i(t) = (z_i - \hat{z}_i(0)) \exp \left(-\frac{\lambda_i+\Gamma}{\tau}t \right) + \frac{\Gamma z_i - \sqrt{\lambda_i}\tilde{\varepsilon}_i}{\lambda_i +\Gamma} \left( 1 - \exp \left(-\frac{\lambda_i+\Gamma}{\tau}t \right) \right)
$$

$$
\begin{aligned}
E_g &= \sigma_e^2 + \mathrm{bias}^2 + \mathrm{variance} \\
    &= \sigma_e^2 + \frac{1}{N} \sum^N_{i=1} \left\langle (z_i - ⟨\hat{z}_i⟩_{\hat{z}_i})^2 \right\rangle _{z_i} + \frac{1}{N} \sum^N_{i=1} \left\langle (\hat{z}_i - ⟨\hat{z}_i⟩_{\hat{z}_i})^2 \right\rangle _{\hat{z}_i, z_i} \\  
    &= \sigma_e^2 + \frac{1}{N} \sum^N_{i=1} \sigma_w^2 e^{-\frac{2(\lambda_i+\Gamma)t}{\tau}} +
    \\ & \frac{1}{N} \sum^N_{i=1} \left[ \sigma_0^2 e^{-\frac{2(\lambda_i+\Gamma)t}{\tau}} + \frac{\Gamma^2 \sigma_w^2 + \lambda_i \sigma_e^2}{(\lambda_i + \Gamma)^2} \left(1 - e^{-\frac{(\lambda_i+\Gamma)t}{\tau}}\right)^2 + \frac{2 \Gamma \sigma_w^2}{\lambda_i + \Gamma} e^{-\frac{(\lambda_i+\Gamma)t}{\tau}} \left(1 - e^{-\frac{(\lambda_i+\Gamma)t}{\tau}}\right) \right]
\end{aligned}
$$

In the limit $t \to \infty$, since $\Gamma + \lambda > 0$,

$$
\frac{E_g(t)}{\sigma_w^2} = \mathrm{ISNR} + \int d\lambda \, \rho^{\mathrm{MP}}(\lambda) \left[ \frac{\Gamma^2 + \lambda\,\mathrm{ISNR}}{(\lambda + \Gamma)^2} \right]
$$

where

$$
\mathrm{ISNR} \equiv \frac{\sigma_e^2}{\sigma_w^2} \quad \mathrm{INR} \equiv \frac{\sigma_0^2}{\sigma_w^2}
$$

#### How does L2-regularization help overfitting?

Without L2-regularization (i.e., $\Gamma=0$), the integrand becomes $\mathrm{ISNR}/\lambda$. By introducing the $\Gamma$ term in the denominator, we are able to supress divergence near $\lambda=0$, and therefore prevent overfitting even at the critical case.

#### Minimizing Generalization Error

Since the integrand is convex, the generalization error is minimum at $Γ=\mathrm{ISNR} = {\sigma_e^2}/{\sigma_w^2}$. Therefore, the larger the noise in the teacher network (compared to the 'correct' weights), the larger the regularization term must be to prevent overfitting.

### Impact Of Initial Weight Size In Generalization

If the student weights have initial variance $\left(\sigma_w^0\right)^2$, then the optimal regularization strength is

$$
\gamma^{\text{opt}} = \frac{\sigma_\varepsilon^2}{(\sigma_w^0)^2 + \sigma_\bar{w}^2}
$$

which then yields

$$
E_g = \frac{(\sigma_w^0)^2 + \sigma_\bar{w}^2}{2}\left( 1 - \alpha - \gamma^{\mathrm{opt}} + \sqrt{(\gamma^{\mathrm{opt}} + \alpha - 1 )^2 + 4 \gamma^{\mathrm{opt}}} \right) + \sigma_\varepsilon^2
$$

in the high limit of $\text{SNR}$, there is a linear dependence on initial weight size when $\alpha < 1$, i.e., overparameterized case

$$
E_g = \left( \left(\sigma^0_w\right)^2 + \sigma_\bar{w}^2 \right)(1-\alpha) + \sigma_\varepsilon^2
$$

when $\alpha < 1$. Therefore, in the overparametrized case, it is critial to initialize with small weights.

#### Training Error Dynamics

It is theoretically shown that the training error goes to zero exponentially in time, even for noisy input data.

## References

[1] Advani, Madhu S., Andrew M. Saxe, and Haim Sompolinsky. 2020. “High-Dimensional Dynamics of Generalization Error in Neural Networks.” Neural Networks: The Official Journal of the International Neural Network Society 132 (December): 428–46.
