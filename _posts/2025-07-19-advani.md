---
title: Generalization Dynamics and Double-Descent Phenomenon In a Simple Neural Network
date: 2025-07-19 20:00:00 +0800
description: Part 1 of summary of Advani et al., Neural Networks 132, 2020, 428-46
categories: [machine-learning-deep-learning, paper-reading]
tags: [paper-reading]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\ww{\textbf{w}}
    \def\xx{\textbf{x}}
    \def\zz{\textbf{z}}
$$


## Overfitting Happens When Model Size is Close To Data Size

The authors first begin with a simple, linear teacher-student setup. The teacher network implements

$$
y^\mu = \ww \cdot \xx^\mu + \varepsilon^\mu
$$

where the superscript represents the data label (a total of $P$ data), and subscript represents the vector component. The weights and values are all vectors of dimension $N$. The values are samples from $\varepsilon^\mu \sim \mathcal{N}(0, \sigma_\varepsilon^2)$, $\ww_i \sim \mathcal{N}(0, \sigma_w^2)$, and $\xx_i^\mu \sim \mathcal{N}(0,1/N)$. The student outputs $\hat{y}^\mu = \hat{\ww}\cdot \xx^\mu$, and tries to minimize

$$
E_{\mathrm{tr}}=\frac{1}{P}\sum^P_{\mu=1} |y^\mu-\hat{y}^\mu|^2
$$

using gradient descent. For simplicity, the matrices $X \equiv [\xx^1 \, \cdots \, \xx^P]$, $y \equiv [y^1 \, \cdots, \, y^P]^T$, $\hat{y} = [\hat{y}^1 \, \cdots \, \hat{y}^P]^T$, and $\varepsilon \equiv [\varepsilon^1 \, \cdots \, \varepsilon^P]^T$ are introduced so that

$$
y=X^Tw+\varepsilon \quad \text{and} \quad \hat{y}=X^T\hat{w}
$$

The gradient descent dynamics are,

$$
\frac{d\hat{\ww}}{dt}=-\gamma\frac{\partial E_{\mathrm{tr}}}{\partial \hat{\ww}}=\frac{2\gamma}{P}X[X^T(\ww-\hat{\ww})+\varepsilon]
$$

Peform eigendecomposition $\Sigma \equiv \frac{1}{P}XX^T = V \Lambda V^T$ where $V$ is orthogonal and $\Lambda$ is a diagonal matrix. Then automatically $X = \sqrt{P} V \Lambda^{1/2} U^T$. Denote $\zz \equiv V^T \ww$ and $\hat{\zz} \equiv V^T \hat{\ww}$. Define $\tau^{-1} \equiv 2\gamma$. Rewriting the dynamics,

$$
\tau \frac{d\hat{\zz}}{dt}=\Lambda (\zz - \hat{\zz})+ \Lambda^{1/2}\frac{U^T\varepsilon}{\sqrt{P}}
$$

Then, $\tilde{\varepsilon} \equiv U^T\varepsilon /\sqrt{P}$ becomes another independent Gaussian variable with standar deviation $\sigma_\varepsilon$. By solving for each component of $\zz$, we obtain the full dynamics.

$$
z_i - \hat{z}_i(t) = [z_i-\hat{z}_i(0)] e^{-\lambda_i t / \tau} - \frac{\tilde{\varepsilon}_i}{\sqrt{\lambda_i}}(1-e^{-\lambda_i t /\tau}) \tag{1}
$$

Therefore, the generalization error $E_g = \left\langle (y-\hat{y})^2 \right\rangle_{y,\hat{y}}$ becomes (with some tedious calculations)

$$
E_g = \sigma_\varepsilon^2+\frac{1}{N}\sum^N_{i=1} \sigma_w^2 e^{-\frac{2\lambda_i t}{\tau}} + \frac{1}{N} \sum^N_{i=1} \left[\hat{\sigma}_0^2 e^{-\frac{2\lambda_i t}{\tau}}+\frac{\sigma_\varepsilon^2}{\lambda_i} \left( 1-e^{-\frac{\lambda_i t}{\tau}} \right)^2 \right]
$$

The first term represents $\mathrm{bias}^2$, and the second term represents the variance. As time $t \to \infty$, the first term decays to zero and the second term exponentially reaches the asymptote of $\sigma_\varepsilon^2 / \lambda_i$. This term corresponds to overfitting when noise is present. 

There are two important factors to note. First, components with _exactly_ zero eigenvalues are 'frozen', i.e., no learning occurs. Therefore, the generalization performance relies heavily on the initialization. Second, components with small-but-positive eigenvalues lead to overfitting. See Eq. (1). When $\lambda_i \ll 1$, the noise parameter $\tilde{\varepsilon}_i$ has a larger impact on the weight dynamics. Hence, the gap between the smallest positive eigenvalue and zero determines overfitting. 


We can theoretically analzye these results by taking $N \to \infty$ while keeping $P/N=\alpha$ fixed. Then the sum over eigenvalues converges to

$$
\frac{1}{N}\sum^N_{i=1} \longrightarrow \int d\lambda \, \rho^{\mathrm{MP}}(\lambda)
$$

where $\rho^{\mathrm{MP}}$ is the *Marchenko-Pastur distribution*:

$$
\rho^{\mathrm{MP}}(\lambda) = \begin{cases} \frac{1}{2\pi}\frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda},&\text{if }\lambda_-\le\lambda\le\lambda_+ \text{ or }\lambda=0 \\ 0, & \text{otherwise} \end{cases}
$$

where $\lambda_{\pm} = \left(\sqrt{\alpha}\pm1\right)^2$. When $\alpha < 1$, the number of parameters $N$ is greater than the number of data $P$, and thus overparameterized. When $\alpha > 1$, the system is underparametrized. When $\alpha = 1$, there is a clear **peak at $\lambda = 0$**. Correspondingly, catastrophic overfitting happens at $\alpha=1$. Indeed, if there are many degrees of freedom with $\lambda$ very close to zero, then the student cannot distinguish noise from signal. 

Therefore, as the number of neurons reach $\infty$, a theoretical peak of generalization error is observed where $P \simeq N$. This phenomenon is called _double-descent_, where the model performs worst when the number of data is similar to the number of parameters, and increases in performance as the number of parameters highly exceeds the number of data.

---

## Ways to combat overfitting

#### Early stopping

The generalization error reaches a maximum at a certain epoch, where the training is stopped. The optimal stopping time is

$$
t^{opt}=\frac{\tau}{\lambda}\log(\mathrm{SNR}\cdot\lambda+1)
$$

where the assumption $w(0) = 0$ is used.

#### L2-regularization

Add a term 
$$\frac{\gamma}{2}||w||_2^2$$
 to the loss function. Then the loss function becomes

$$
E_{\mathrm{tr}} = \frac{1}{P} \sum^P_{\mu=1} |y^\mu - \hat{y}^\mu|^2 + \Gamma \sum^N_{i=1} \hat{w}_i^2 = \frac{1}{P}\left|\mathbf{X}^T(\mathbf{w}-\mathbf{\hat{w}}) + \mathbf{\epsilon}\right|^2 + \Gamma |\mathbf{\hat{w}}|^2
$$

and the resulting gradient descent dynamics become

$$
\tau \frac{d\hat{z}_i}{dt} = \lambda_i(z_i - \hat{z}_i) + \sqrt{\lambda_i} \tilde{\varepsilon}_i -\Gamma \hat{z}_i
$$

Therefore, the integrated dynamics are

$$
z_i - \hat{z}_i(t) = (z_i - \hat{z}_i(0)) \exp \left(-\frac{\lambda_i+\Gamma}{\tau}t \right) + \frac{\Gamma z_i - \sqrt{\lambda_i}\tilde{\varepsilon}_i}{\lambda_i +\Gamma} \left( 1 - \exp \left(-\frac{\lambda_i+\Gamma}{\tau}t \right) \right)
$$

$$
\begin{aligned}
E_g &= \sigma_e^2 + \mathrm{bias}^2 + \mathrm{variance} \\
    &= \sigma_e^2 + \frac{1}{N} \sum^N_{i=1} \left\langle (z_i - ⟨\hat{z}_i⟩_{\hat{z}_i})^2 \right\rangle_{z_i} + \frac{1}{N} \sum^N_{i=1} \left\langle (\hat{z}_i - ⟨\hat{z}_i⟩_{\hat{z}_i})^2 \right\rangle_{\hat{z}_i, z_i} \\  
    &= \sigma_e^2 + \frac{1}{N} \sum^N_{i=1} \sigma_w^2 e^{-\frac{2(\lambda_i+\Gamma)t}{\tau}} + 
    \\ & \frac{1}{N} \sum^N_{i=1} \left[ \sigma_0^2 e^{-\frac{2(\lambda_i+\Gamma)t}{\tau}} + \frac{\Gamma^2 \sigma_w^2 + \lambda_i \sigma_e^2}{(\lambda_i + \Gamma)^2} \left(1 - e^{-\frac{(\lambda_i+\Gamma)t}{\tau}}\right)^2 + \frac{2 \Gamma \sigma_w^2}{\lambda_i + \Gamma} e^{-\frac{(\lambda_i+\Gamma)t}{\tau}} \left(1 - e^{-\frac{(\lambda_i+\Gamma)t}{\tau}}\right) \right]
\end{aligned}
$$

In the limit $t \to \infty$, since $\Gamma + \lambda > 0$,

$$
\frac{E_g(t)}{\sigma_w^2} = \mathrm{ISNR} + \int d\lambda \, \rho^{\mathrm{MP}}(\lambda) \left[ \frac{\Gamma^2 + \lambda\,\mathrm{ISNR}}{(\lambda + \Gamma)^2} \right]
$$

where

$$
\mathrm{ISNR} \equiv \frac{\sigma_e^2}{\sigma_w^2} \quad \mathrm{INR} \equiv \frac{\sigma_0^2}{\sigma_w^2}
$$

**How does L2-regularization help overfitting?**

Without L2-regularization (i.e., $\Gamma=0$), the integrand becomes $\mathrm{ISNR}/\lambda$. By introducing the $\Gamma$ term in the denominator, we are able to supress divergence near $\lambda=0$, and therefore prevent overfitting even at the critical case.

**Minimizing Generalization Error**

Since the integrand is convex, the generalization error is minimum at $Γ=\mathrm{ISNR} = {\sigma_e^2}/{\sigma_w^2}$. Therefore, the larger the noise in the teacher network (compared to the 'correct' weights), the larger the regularization term must be to prevent overfitting.

#### Impact Of Initial Weight Size In Generalization

If the student weights have initial variance $\left(\sigma_w^0\right)^2$, then the optimal regularization strength is $\gamma^{\text{opt}} = \sigma_\varepsilon^2 / \{(\sigma_w^0)^2 + \sigma_\bar{w}^2 \}$, which then yields

$$
E_g = \frac{(\sigma_w^0)^2 + \sigma_\bar{w}^2}{2}\left( 1 - \alpha - \gamma^{\mathrm{opt}} + \sqrt{(\gamma^{\mathrm{opt}} + \alpha - 1 )^2 + 4 \gamma^{\mathrm{opt}}} \right) + \sigma_\varepsilon^2
$$

in the high limit of $\text{SNR}$, there is a linear dependence on initial weight size when $\alpha < 1$, i.e., overparameterized case: $E_g = \left( \left(\sigma^0_w\right)^2 + \sigma_\bar{w}^2 \right)(1-\alpha) + \sigma_\varepsilon^2$ when $\alpha < 1$. Therefore, in the overparametrized case, it is critial to initialize with small weights.

#### Training Error Dynamics

_TODO: write out the results_ A summary: it is theoretically shown that the training error goes to zero exponentially in time, even for noisy input data.

## References
[1] Advani, Madhu S., Andrew M. Saxe, and Haim Sompolinsky. 2020. “High-Dimensional Dynamics of Generalization Error in Neural Networks.” Neural Networks: The Official Journal of the International Neural Network Society 132 (December): 428–46.