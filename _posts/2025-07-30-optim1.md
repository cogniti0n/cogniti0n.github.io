---
title: Optimization for Machine Learning
date: 2025-07-30 05:50 +0800
description: Part 1 on optimization for ML. 
categories: [optimization]
tags: [least-squares]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\R{\mathbb{R}}
$$

## Overview

Optimization is a rich field with prolonged history. Classically, the purpose of optimization is to minimize an objective function over a domain (or constraints). In classical (non-deep) machine learning, such optimization schemes fit well into the argument of empirical risk minimization, where the expected risk can be bounded above by methods such as using Rademacher complexity. In deep learning, such bounds are often too loose compared to empirical evidence of generalization, and often there is some sort of implicit bias towards a more generalizable minima (dependent on hyperparamters and optimizers). 

The contents of this series will consist of various topics ranging from classical convex optimization (mostly from [1] and [2]) to theories such as NTK, and modern optimizers (adagrad, ADAM, etc.).

## Preliminaries

_I follow Ryu and Yin's notations[2] - in particular, CCP means closed, convex, and proper._

$g \in \R^n$ is a _subgradient_ of convex $f$ if

$$
f(y) \ge f(x) + g^T (y-x)
$$

The subdifferential $\partial f(x)$ is the set of all subgradients of $f$ at $x$. $\partial f$ is closed and convex, as it is the intersection of 
$$\{g | f(y) \ge f(x) + g^T (y-x) \}$$ 
for a fixed $y$, which is closed and convex. If $f$ is differentiable at $x$, then $\partial f$ is a singleton. Intuitively we can predict that $x^* \in \argmin \, f$ if and only if $0 \in \partial f (x^* )$. 

The following holds (identities with regularity conditions)

- $\partial (\alpha f) = \alpha (\partial f)$ if $\alpha > 0$
- $g(x) = f(Ax)$, then $\partial g = A^\top \partial f(Ax)$, if $\mathcal{R}(A) \cap \text{ri dom} \, f \ne \emptyset$
- $\partial (f + g) = \partial f + \partial g$ if $\mathrm{dom} \, f \cap \mathrm{dom} \, g \ne \emptyset$

Such regularity conditions mirror that of the Slater conditions with strong duality. 

> **Definition** (Strong Convexity) A CCP function $f$ is $\mu$-strongly convex if either of these conditions hold (they are equivalent)
> - $f(x) - (\mu/2) \|x\|^2$ is convex
> - $\left\langle \partial f(x) - \partial f(y), x-y \right\rangle \ge \mu \|x-y\|^2$, $\forall x,y$ (overloaded set notation: more specifically, $\langle u-v, x-y \rangle \dots$, for all $u \in \partial f(x)$, $v \in \partial f(y)$)
> - $\nabla^2 f(x) \succeq \mu I$, $\forall x$ if $f$ is twice-differentiable.

Strong convexity is closed under addition.

> **Definition** (Smoothness) A CCP function $f$ is $L$-smooth if
> - $f(x) - (L/2) \|x\|^2$ is concave.
> - $f$ is differentiable and  $\left\langle \nabla f(x) - \nabla f(y), x-y \right\rangle \ge (1/L) \| \nabla f(x) - \nabla f(y) \|^2$, $\forall x,y$ (cocoercivity inequality)
> - $f$ is differentiable and $\nabla f$ is $L$-Lipschitz (Baillou-Haddad theorem)
> - If $f$ is twice differentiable, then $\nabla^2 f(x) \preceq LI$. 

Then if $f$ is $\mu$-strongly convex and $L$-smoooth, $\mu \le L$. Also, if $f$ is CCP, then $f$ is $\mu$-strongly convex if and only if $f^* $ is $1/\mu$-smooth.

> **Definition** (Proximal Operator) The proximal operator with respect to $\alpha f$ is defined as
>
> $$
\mathrm{Prox}_{\alpha f} \, (y) = \argmin_{x \in \R^n} \, \left\{ \alpha f(x) + \frac{1}{2} \| x - y \|^2 \right\}
> $$
> for CCP $f$ and $\alpha > 0$. 

If $f$ is CCP, then the proximal operator is well-defined.

The operator 

$$
S(x;\kappa) = \mathrm{Prox}_{\kappa \| \cdot \|_1}(x)
$$

is the _soft-thresholding_ operator. If the input $x$ satisfies $ \|x\| > \kappa$, then the output adds or subtracts $\kappa$ accordingly. 

Another common example is the _projection_ operator. Given a nonempty closed convex set $C \subseteq \R^n$, then the projection operator is

$$
\Pi_C(y) = \argmin_{x \in C} \, \| x-y \|
$$

The projection is a proximal operator $\mathrm{Prox}_{\alpha \delta_C}$ for any $\alpha > 0$. 

Evaluating the proximal operator is itself an optimization problem, but many interesting convex $f$ has solutions for $\mathrm{Prox}_{\alpha f}$ - which are called 'proximable'. We decompose an optimization problem into smaller, simpler differentiable or proximable functions.

## References
[1] Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge, England: Cambridge University Press.\
[2] Ryu, Ernest K., and Wotao Yin. 2022. Large-Scale Convex Optimization. Cambridge, England: Cambridge University Press.
