---
title: Overview & Preliminaries In Convex Optimization
date: 2025-07-30 05:50 +0800
description: Overview and preliminary definitions in optimization theory.
categories: [optimization, convex-optimization]
tags: [lecture-notes]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## Overview

Optimization is a rich field with prolonged history. Classically, the purpose of optimization is to minimize an objective function over a domain (or constraints). In classical (non-deep) machine learning, such optimization schemes fit well into the argument of empirical risk minimization, where the expected risk can be bounded above by methods such as using Rademacher complexity. In deep learning, such bounds are often too loose compared to empirical evidence of generalization, and often there is some sort of implicit bias towards a more generalizable minima (dependent on hyperparamters and optimizers). 

The contents of this series will consist of various topics ranging from classical convex optimization (mostly from [1], [2], and [3]) to theories such as NTK, and modern optimizers (adagrad, ADAM, etc.).

## Preliminaries

_I follow Ryu and Yin's notations[2] - in particular, CCP means closed, convex, and proper._

> **Definition** The _domain_ of $f$ is defined as 
> $$\dom{f} = \{ x \in \R^n | f(x) < \infty \}$$.

A function $f$ is convex if $\dom{f}$ is convex and

$$
f(\theta x + (1-\theta) x) \le \theta f(x) + (1-\theta) f(x) \quad \forall x,y \in \dom{f}, \, \theta \in (0,1)
$$


$g \in \R^n$ is a _subgradient_ of convex $f$ if

$$
f(y) \ge f(x) + g^T (y-x)
$$

> **Definition** (Indicator Function) For $S \subseteq \R^n$, define the indicator function
>
> $$
\delta_S(x) = \begin{cases} 0 & \text{if } x \in S \\ \infty & \text{otherwise} \end{cases}
> $$

The indicator function is regularly used to denote implicit domains. For instance, consider minimizing $f(x)$ under the condition $x \in C \subseteq \dom{f}$. Then we transform the problem into minimizing $f(x) + \delta_C(x)$ where $x \in \dom{f}$.

The subdifferential $\partial f(x)$ is the set of all subgradients of $f$ at $x$. $\partial f$ is closed and convex, as it is the intersection of 
$$\{g | f(y) \ge f(x) + g^T (y-x) \}$$ 
for a fixed $y$, which is closed and convex. If $f$ is differentiable at $x$, then $\partial f$ is a singleton. Intuitively we can predict that $x^* \in \argmin \, f$ if and only if $0 \in \partial f (x^* )$. 

The following holds (identities with regularity conditions)

- $\partial (\alpha f) = \alpha (\partial f)$ if $\alpha > 0$
- $g(x) = f(Ax)$, then $\partial g = A^\top \partial f(Ax)$, if $\mathcal{R}(A) \cap \text{ri dom} \, f \ne \emptyset$
- $\partial (f + g) = \partial f + \partial g$ if $\mathrm{dom} \, f \cap \mathrm{dom} \, g \ne \emptyset$

Such regularity conditions mirror that of the Slater conditions with strong duality. 

> **Definition** (Strong Convexity) A CCP function $f$ is $\mu$-strongly convex if either of these conditions hold (they are equivalent)
> - $f(x) - (\mu/2) \|x\|^2$ is convex
> - $\left\langle \partial f(x) - \partial f(y), x-y \right\rangle \ge \mu \|x-y\|^2$, $\forall x,y$ (overloaded set notation: more specifically, $\langle u-v, x-y \rangle \dots$, for all $u \in \partial f(x)$, $v \in \partial f(y)$)
> - $\nabla^2 f(x) \succeq \mu I$, $\forall x$ if $f$ is twice-differentiable.

Strong convexity is closed under addition.

> **Definition** (Smoothness) A CCP function $f$ is $L$-smooth if
> - $f(x) - (L/2) \|x\|^2$ is concave.
> - $f$ is differentiable and  $\left\langle \nabla f(x) - \nabla f(y), x-y \right\rangle \ge (1/L) \| \nabla f(x) - \nabla f(y) \|^2 \quad \forall x,y$ (cocoercivity inequality)
> - $f$ is differentiable and $\nabla f$ is $L$-Lipschitz (Baillou-Haddad theorem)
> - If $f$ is twice differentiable, then $\nabla^2 f(x) \preceq LI$. 

Then if $f$ is $\mu$-strongly convex and $L$-smoooth, $\mu \le L$. 

> **Proposition** If $f$ is CCP, then $f$ is $\mu$-strongly convex if and only if $f^* $ is $1/\mu$-smooth.

> **Definition** (Proximal Operator) The proximal operator with respect to $\alpha f$ is defined as
>
> $$
\mathrm{Prox}_{\alpha f} \, (y) = \argmin_{x \in \R^n} \, \left\{ \alpha f(x) + \frac{1}{2} \| x - y \|^2 \right\}
> $$
> for CCP $f$ and $\alpha > 0$. 

If $f$ is CCP, then the proximal operator is well-defined.

The operator 

$$
S(x;\kappa) = \mathrm{Prox}_{\kappa \| \cdot \|_1}(x)
$$

is the _soft-thresholding_ operator. If the input $x$ satisfies $ \|x\| > \kappa$, then the output adds or subtracts $\kappa$ accordingly. 

Another common example is the _projection_ operator. Given a nonempty closed convex set $C \subseteq \R^n$, then the projection operator is

$$
\Pi_C(y) = \argmin_{x \in C} \, \| x-y \|
$$

The projection is a proximal operator $\mathrm{Prox}_{\alpha \delta_C}$ for any $\alpha > 0$. 

Evaluating the proximal operator is itself an optimization problem, but many interesting convex $f$ has solutions for $\mathrm{Prox}_{\alpha f}$ - which are called 'proximable'. We decompose an optimization problem into smaller, simpler differentiable or proximable functions.

In [2], operators are generalized to multi-valued functions, and therefore an operator $T: \R^n \rightrightarrows \R^n$ (for the sake of $\TeX$ conciseness, I won't write $\mathbb{T}$ every time for a multi-valued operator) maps a single input value in $\R^n$ to a set $\subseteq \R^n$. The _graph_ of an operator is defined as 
$$\mathrm{Gra}\,T = \left\{(x,u) \middle| u \in T(x) \right\}$$
. The sum of two operators is the Mincowski sum. If we write an equation such as (the definition of Lipschitzness)

$$
\| Tx - Ty \| \le L \| x-y \| \quad \forall x,y \in \dom{T}
$$

then the actual meaning is as follows:

$$
\left( \| u-v \| \le L \| x-y \| \quad \forall u \in T(x), \, v \in T(y) \right) \quad \forall x,y \in \dom{T}
$$

If $T$ is Lipschitz, then it is single-valued.

Because operators are multi-valued, the inverse is always well-defined. The zeros of an operator $T$ is therefore defined as the set $T^{-1}(0)$. We now prove an elegent result regarding the subdifferential using this notion of zeros.

> **Theorem** (Subdifferential of Conjugates) If $f$ is CCP, then
>
> $$
(\partial f)^{-1} = \partial f^*
> $$

_proof_. Since $f$ is CCP, $f^** = f$, and $x \in \argmin_{z}{f(z)} \Leftrightarrow 0 \in \partial f$.

$$
\begin{aligned}
u \in \partial f(x) &\Leftrightarrow 0 \in \partial f(x) - u \\
                    &\Leftrightarrow x \in \argmin_{z}\left\{ f(z) - u^\top z \right\} \\
                    &\Leftrightarrow f(x) - u^\top z = - f^* (u) \\
                    &\Leftrightarrow f(x) + f^*(u) = u^\top x \\
                    &\Leftrightarrow f^{**} (x) + f^* (u) = u^\top x \\
                    &\Leftrightarrow x \in \partial f^* (u)
\end{aligned}
$$

Where the last step holds because we can take back the whole process backwards with $f^* $ in place of $f$ and $u$, $x$ switched. From this nice property, we show the following theorem.

> **Theorem** Take $g(y) = f^* (A^\top y)$, where $f$ is CCP and $\mathcal{R}(A^\top) \cap \mathrm{ri}\,\dom{f} \ne \emptyset$. Then the following holds.
>
> $$
u \in \partial g(y) \Leftrightarrow u = Ax \text{ for } x \in \argmin_z \left\{ f(z) - \langle y, Az \rangle \right\}
> $$

> **Definition** (Monotone Operators) An operator $T$ is monotone if it satisfies
>
> $$
\left\langle T(x) - T(y), x-y \right\rangle \ge 0 \quad \forall x,y \in \dom\,{T}
> $$
> 
> Moreover, $T$ is _maximally monotone_ if $\nexists x,u$ such that $T \cup \{ (x,u) \}$ is monotone

Again, there is abuse of notation - the real meaning is that the inequality holds for all $u \in T(x)$ and $v \in T(y)$. I've also written $\mathrm{Gra}\,T$ as just $T$. A nice property is as follows.

> **Proposition** If $f$ is CCP, then $\partial f$ is maximally monotone.

The proof is easily shown using the definitions.

> **Definition** (Cocoersivity) An operator $A: \R^n \rightrightarrows \R^n$ is $\mu$-coersive, or $\mu$-strongly monotone if
>
> $$
\left\langle T(x)-T(y), x-y \right\rangle \ge \frac{\mu}{2} \| x-y \|^2
> $$
>
> An operator $A: \R^n \rightrightarrows \R^n$ is $\beta$-cocoersive, or $\beta$-inverse strongly monotone if
> 
> $$
\langle u-v, x-y \rangle \ge \beta \| u-v \|^2 \quad \forall (x,u), (y,v) \in A
> $$

The two properties are inverse in the sense that $A$ is $\beta$-cocoersive if and only if $A^{-1}$ is $\beta$-strongly monotone. For a $\beta$-cocoersive operator $A$, 

$$
\frac{1}{\beta}\| x-y \|^2 \ge \| Ax - Ay \| \quad  \forall x,y \in \R^n
$$

and therefore is $1/\beta$-Lipschitz. Correspondingly, a cocoersive operator is single-valued. A useful fact is that, if $A$ is cocoersive, then $A$ is maximal if and only if $\dom{A} = \R^n$. Hereinafter, "An operator $A : \R^n \to \R^n$ is $\beta$-cocoersive" implictly asserts that $\dom{A} = \R^n$ and $A$ is maximal.

> **Proposition** Let $f$ be a CCP function. Then the following holds.
> - $f$ is $\mu$-strongly convex $\Leftrightarrow$ $\partial f$ is $\mu$-strongly monotone.
> - $f$ is $L$-smooth $\Leftrightarrow$ $\partial f$ is $L$-Lipschitz $\Leftrightarrow$ $\partial f$ is $1/L$-cocoersive

Be aware that in general, Lipschitzness does not imply cocoersivity.

Just like we consider operators that preserve convexity (see chapter 2 & 3 of [1]), we now consider operators that preserve mononicity. 

- If $T(x)$ is monotone, then $y + \alpha T(x+z) $ is monotone for $\alpha > 0$
- If $T$ is maximally monotone, then $T^{-1}$ is also maximally monotone
- If $T$ and $S$ is monotone, then $T+S$ is monotone
- Moreover, if $T$ and $S$ are maximally monotone and regularity conditions $\dom\,{T} \cap \mathrm{int}\,\dom\,{S} \ne \emptyset$ holds, then $T+S$ is maximally monotone.
- If $T: \R^n \rightrightarrows \R^n$ is monotone, and $M \in \R^{n\times n}$, then $M^\top T M$ is monotone.
- If $R: \R^n \rightrightarrows \R^n$ and $S: \R^n \rightrightarrows \R^n$ are (maximal) monotone, then the concatenation

$$
T(x,y) = \{ (u,v) \, | \, u \in R(x), \, v \in S(y) \}
$$

is (maximal) monotone. We use the notation

$$
T(x,y) = \begin{bmatrix} R(x) \\ S(y) \end{bmatrix}
$$

There are also operations preserving strong monotonicity and Lipschitzness

- If $T$ is $\alpha$-s.m. then $\alpha T$ is $\alpha\mu$-s.m.
- If $T$, $S$ are $\mu$-s.m. then $T+S$ is $\mu$-s.m.
- If $T : \R^n \rightrightarrows \R^n$ is $\mu$-s.m., and $M \in \R^{n \times m}$ has rank-$m$, then $M^\top T M$ is $\mu \sigma_{\mathrm{min}}^2(M)$-s.m.
- If $T : \R^n \rightrightarrows \R^n$ is $L$-Lipschitz and $M \in \R^{n \times m}$, then $M^\top T M$ is $L\sigma^2_{\mathrm{max}}(M)$-Lipschitz

## References
[1] Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge, England: Cambridge University Press.\
[2] Ryu, Ernest K., and Wotao Yin. 2022. Large-Scale Convex Optimization. Cambridge, England: Cambridge University Press. \
[3] Bach, Francis. 2024. Learning Theory from First Principles. London, England: MIT Press.
