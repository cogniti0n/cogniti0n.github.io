---
title: Concentration Inequalities
date: 2025-07-15 20:00:00 +0800
description: Part 1 of notes on C229M. A concise arrangement on the concentration inequalities used in the lectures.
categories: [ML Theory (C229M T. Ma)]
tags: [mldl]
math: true
toc: false
---

*TODO: write out the proofs (I am way too lazy)*

>**Theorem** (Central Limit Theorem) Let $X_1, \dots, X_n$ be $i.i.d.$ random variables. Then, $\hat{X} = \frac{1}{n}\sum_i X_i$ satisfies the following as $n \to \infty$. \
1) $\hat{X} \overset{p}{\to} \mathbb{E}[X]$ \
2) $\sqrt{n} (\hat{X} - \mathbb{E}[X]) \overset{d}{\to} \mathcal{N}(0,\Sigma)$ where $\Sigma$ is finite.

>**Theorem** (Chebyshev’s Inequality) Let $X_1,\dots,X_n$ be independent random variables. Let $Z = X_1 + \cdots +X_n$. Then the following holds.
>
>$$
\mathrm{Pr}[|Z-\mathbb{E}[Z]| \ge t] \le \frac{\mathrm{var}[Z]}{t^2}
>$$

>**Definition** (sub-Gaussian) A random variable $X$ with finite mean $\mu = \mathbb{E}[X]$ is called sub-Gaussian with variance proxy $\sigma$ if
>
>$$
\mathbb{E}[e^{\lambda(x-\mu)}] \le e^{\sigma^2 \lambda^2 / 2} \quad \text{for } \forall \lambda \in \mathbb{R}
>$$

Informally, this doesn’t look similar to Gaussian distributions. Here’s a corollary to show the connection.

>**Corollary** If $X$ is sub-Gaussian with v.p. $\sigma$, then
>
>$$
\mathrm{Pr} [|X-\mu| \ge t] \le 2 \exp\left( -\frac{t^2}{2\sigma^2} \right)
>$$

>**Theorem** If $X_1,\dots,X_n$ are independent sub-Gaussian random variables with variance proxy $\sigma_1^2,\dots,\sigma_n^2$ respectively, then $Z = X_1 + \cdots + X_n$ is sub-Gaussian with variance proxy $\sum_i \sigma_i^2$.

>**Theorem** If $|X - \mathbb{E}X| \le M$, then $X$ is $O(M)$ sub-Gaussian. More specifically, If $A \le X \le B$ a.s., then 
$$\mathbb{E} [e^{\lambda(X-\mathbb{E}X)}] \le e^{\lambda^2(B-A)^2/8}$$.

>**Theorem** (Hoeffding’s Inequality) Let $X_1, \dots, X_n$ be independent random variables. Suppose $a_i < X_i < b_i$ almost surely. Let $\mu = \mathop{\mathbb{E}}\left[\frac{1}{n}\sum_i X_i \right]$, then for $\forall \varepsilon > 0$, the following holds.
>
>$$
\mathrm{Pr} \left[ \left|\frac{1}{n} \sum^n_{i=1} X_i - \mu\right| \le \varepsilon \right] \ge 1 - 2 \exp\left( -\frac{2n^2\varepsilon^2}{\sum_i |b_i - a_i|^2} \right)
>$$

Informally speaking, the probability that the empirical mean is close to the expectation of the mean is exponentially large.

>**Theorem** (McDiarmid’s Inequality) Suppose $f$ satisfy the *bounded difference condition*, i.e.,
>
>$$
\forall x_1,\dots,x_n\quad \forall i \quad |f(x_1,\dots,x_i,\dots,x_n) - f(x_1\dots, x_i',\dots,x_n)| \le c_i
>$$
>
>Let $X_1,\dots,X_n$ be independent random variables, then,
>
>$$
\mathrm{Pr}[f(X_1,\dots,X_n)-\mathbb{E}f(X_1,\dots,X_n) \ge t] \le \exp\left(-\frac{2t^2}{\sum_i c_i^2}\right)
>$$

i.e., $f$ is sub-Gaussian with variance proxy $O\left(\sum_i c_i^2\right)$. Here’s a stronger version, which is quite difficult to prove (Thm. 3.18 in van Handel)

>**Theorem** Suppose $f$ satisfies the following condition 
$$D_i^-f(x) \equiv f - \inf_zf(x_1,\dots,x_{i-1},z,x_{i+1},\dots,x_n) \ge 0$$
>
>$$
D_i^+f(x) = \sup_z f(x_1,\dots,z,\dots,x_n)-f \ge 0
>$$
>
>Also denote
>
>$$
d^+ = \sup_{x}\left[ \sum^n_{i=1} |D_i^+f(x)|^2 \right]
>$$
>
>$$
d^- = \sup_{x}\left[ \sum^n_{i=1} |D_i^-f(x)|^2 \right]
>$$
>
>Then the following inequalities hold.
>
>$$
\mathrm{Pr}[f(x) - \mathbb{E}f(x) \ge t] \le \exp\left(-\frac{t^2}{4d^-}\right)
>$$
>
>$$
\mathrm{Pr}[f(x) - \mathbb{E}f(x) \le -t] \le \exp\left(-\frac{t^2}{4d^+}\right)
>$$

## References

[1] Tengyu Ma, Lecture notes for “C229M, Machine Learning Theory”.