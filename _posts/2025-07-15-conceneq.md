---
title: Concentration Inequalities
date: 2025-07-15 20:00:00 +0800
description: Part 1 of notes on C229M. A concise arrangement on the concentration inequalities used in the lectures.
categories: [machine-learning-deep-learning, classic-ml-theory]
tags: [lecture-notes]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
$$

*TODO: write out the proofs (I am way too lazy)*

>**Theorem** (Central Limit Theorem) Let $X_1, \dots, X_n$ be $i.i.d.$ random variables with mean $\mu$ and variance $\sigma^2 < \infty$. Define the random variable $$\bar{X}_n = \frac{1}{n} \sum^n_{i=1} X_i$$. Then as $n \to \infty$, the following holds.
>
> $$
\sqrt{n}(\bar{X}_n - \mu) \overset{d}{\to} \mathcal{N}(0,\sigma^2)
> $$
>
> The same holds for multidimensional random variables with finite covariance.

*proof*. Normalize the random variables as $Y_i \equiv (X_i - \mu)/\sigma$. Now define a new random variable

$$
Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} = \sum^n_{i=1} \frac{1}{\sqrt{n}} Y_i
$$

Since $Y_i$ are i.i.d. distributed over a standard Gaussian distribution, the characteristic functions of each $Y_i$, denoted $\phi_{Y_i}(t)$, are identical. From Taylor's theorem we expand

$$
\phi_{Y_i}(t) = 1 - \frac{t^2}{2} + o\left({t^2}\right)
$$

Therefore, the characteristic function of $Z_n$ becomes

$$
\phi_{Z_n}(t) = \prod^n_{i=1} \phi_{Y_i}\left(\frac{t}{\sqrt{n}}\right) = \left( 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right) \right)^n \to e^{-\frac{1}{2}t^2}, \quad n \to \infty
$$

As the limit $n \to \infty$ is taken, all the higher orders vanish. Therefore, the sequence of characteristic functions $\phi_{Z_n}$ converges pointwise to the characteristic function of a standard Gaussian distribution. From Lévy's continuity theorem, the random variables $Z_n \overset{d}{\to} \mathcal{N}(0,1)$. The proof is complete.

>**Theorem** (Markov's Inequality) If $X$ is a nonnegative random variable and $a > 0$, then
>
>$$
\mathrm{Pr}[X \ge a] \le \frac{\expectation(X)}{a}
>$$

*proof*. Use $$\mathbf{1}\{X \ge \varepsilon\} \le \frac{1}{\varepsilon}X$$, then we conclude

$$
\mathrm{Pr}[X \ge \varepsilon] = \expectation[\mathbf{1}\{X \ge \varepsilon\}] \le \expectation\left[\frac{1}{\varepsilon}X \right]
$$

>**Theorem** (Chebyshev’s Inequality) Let $X_1,\dots,X_n$ be independent random variables with mean $\mu$ and variance $\sigma^2$. Let $\bar{X} = \frac{1}{N}( X_1 + \cdots + X_N )$. Then the following holds.
>
>$$
\mathrm{Pr}[|\bar{X}-\mu| \ge \varepsilon] \le \frac{\sigma^2}{N \varepsilon^2}
>$$

_proof_. Use Markov's inequality.

$$
\mathrm{Pr}[|\bar{X}-\mu| \ge \varepsilon] = \mathrm{Pr}[(\bar{X}-\mu)^2 \ge \varepsilon^2] \le \frac{\sigma^2}{N \varepsilon^2}
$$

>**Definition** (sub-Gaussian) A random variable $X$ with finite mean $\mu = \mathbb{E}[X]$ is called sub-Gaussian with variance proxy $\sigma$ if
>
>$$
\mathbb{E}[e^{\lambda(X-\mu)}] \le e^{\sigma^2 \lambda^2 / 2} \quad \text{for } \forall \lambda \in \mathbb{R}
>$$

Inforamally, sub-Gaussian distributions have tails upper bounded by Gaussian distributions, hence the *sub-* prefix. The following corollary directly shows such tail bound.

>**Corollary** If $X$ is sub-Gaussian with variance proxy $\sigma$, then
>
>$$
\mathrm{Pr} [|X-\mu| \ge t] \le 2 \exp\left( -\frac{t^2}{2\sigma^2} \right)
>$$

*proof*. Fix $t > 0$. For any $\lambda > 0$,

$$
\begin{aligned}
\mathrm{Pr}[X - \mu \ge t] &= \mathrm{Pr}\left[e^{\lambda(X-\mu)} \ge e^{\lambda t}\right] \\
                           &\le e^{-\lambda t} \expectation\left[ e^{\lambda(X-\mu)} \right], \quad \text{(by Markov's inequality)} \\
                           &\le e^{-\lambda t} e^{\sigma^2 \lambda^2 / 2}, \quad \text{(by definition of sub-Gaussian)} \\
                           &= \exp(-\lambda t + \sigma^2 \lambda^2 / 2)
\end{aligned}
$$

Since the inequality holds for any $\lambda > 0$, we minimize the right hand side with respect to $\lambda$. Therefore, we have the tight bound

$$
\mathrm{Pr}[X - \mu \ge t] \le \exp\left(-\frac{t^2}{2\sigma^2}\right)
$$

Similarly, we can bound the opposite tail as

$$
\mathrm{Pr}[X - \mu \le -t] \le \exp\left(-\frac{t^2}{2\sigma^2}\right)
$$

Therefore, we have the tail bound on both sides as stated above. The proof is complete.

>**Theorem** If $X_1,\dots,X_n$ are independent sub-Gaussian random variables with variance proxy $\sigma_1^2,\dots,\sigma_n^2$ respectively, then $Z = X_1 + \cdots + X_n$ is sub-Gaussian with variance proxy $\sum_i \sigma_i^2$.

*proof*. Use the independence of $X_i$. Then we can separate

$$
\expectation\left[\exp\{\lambda(Z - \expectation Z)\}\right] = \expectation\left[ \prod^n_{i=1} \exp\{ \lambda(X_i - \expectation X_i) \} \right]
$$

The rest of the proof is clear.

>**Theorem** (Hoeffding's Lemma) If $X$ is a random variable such that $A \le X \le B$ almost surely, then
>
>$$
\mathbb{E} [e^{\lambda(X-\mathbb{E}X)}] \le e^{\lambda^2(B-A)^2/8}
>$$

First, we can shift each variable such that $\expectation X = 0$, and $A \le 0 \le B$. We use the fact that $e^{\lambda x}$ is convex.

>**Theorem** (Hoeffding’s Inequality) Let $X_1, \dots, X_n$ be independent random variables. Suppose $a_i < X_i < b_i$ almost surely. Let $\mu = \mathop{\mathbb{E}}\left[\frac{1}{n}\sum_i X_i \right]$, then for $\forall \varepsilon > 0$, the following holds.
>
>$$
\mathrm{Pr} \left[ \left|\frac{1}{n} \sum^n_{i=1} X_i - \mu\right| \le \varepsilon \right] \ge 1 - 2 \exp\left( -\frac{2n^2\varepsilon^2}{\sum_i |b_i - a_i|^2} \right)
>$$

Informally speaking, the probability that the empirical mean is close to the expectation of the mean is exponentially large.

>**Theorem** (McDiarmid’s Inequality) Suppose $f$ satisfy the *bounded difference condition*, i.e.,
>
>$$
\forall x_1,\dots,x_n\quad \forall i \quad |f(x_1,\dots,x_i,\dots,x_n) - f(x_1\dots, x_i',\dots,x_n)| \le c_i
>$$
>
>Let $X_1,\dots,X_n$ be independent random variables, then,
>
>$$
\mathrm{Pr}[f(X_1,\dots,X_n)-\mathbb{E}f(X_1,\dots,X_n) \ge t] \le \exp\left(-\frac{2t^2}{\sum_i c_i^2}\right)
>$$

i.e., $f$ is sub-Gaussian with variance proxy $O\left(\sum_i c_i^2\right)$. Here’s a stronger version, which is quite difficult to prove (Thm. 3.18 in van Handel)

>**Theorem** Suppose $f$ satisfies the following condition 
$$D_i^-f(x) \equiv f - \inf_zf(x_1,\dots,x_{i-1},z,x_{i+1},\dots,x_n) \ge 0$$
>
>$$
D_i^+f(x) = \sup_z f(x_1,\dots,z,\dots,x_n)-f \ge 0
>$$
>
>Also denote
>
>$$
d^+ = \sup_{x}\left[ \sum^n_{i=1} |D_i^+f(x)|^2 \right]
>$$
>
>$$
d^- = \sup_{x}\left[ \sum^n_{i=1} |D_i^-f(x)|^2 \right]
>$$
>
>Then the following inequalities hold.
>
>$$
\mathrm{Pr}[f(x) - \mathbb{E}f(x) \ge t] \le \exp\left(-\frac{t^2}{4d^-}\right)
>$$
>
>$$
\mathrm{Pr}[f(x) - \mathbb{E}f(x) \le -t] \le \exp\left(-\frac{t^2}{4d^+}\right)
>$$

## References

[1] Ma, Tengyu. 2022. Lecture notes for CS229M (Machine Learning Theory) \
[2] Bach, Francis. 2024. Learning Theory from First Principles. London, England: MIT Press.
