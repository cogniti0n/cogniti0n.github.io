---
title: Concentration Inequalities
date: 2025-07-15 20:00:00 +0800
description: Part 1 of notes on C229M. A concise arrangement on the concentration inequalities used in the lectures.
categories: [machine-learning-deep-learning, classic-ml-theory]
tags: [lecture-notes]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\R{\mathbb{R}}
$$

>**Theorem** (Central Limit Theorem) Let $X_1, \dots, X_n$ be $i.i.d.$ random variables with mean $\mu$ and variance $\sigma^2 < \infty$. Define the random variable $$\bar{X}_n = \frac{1}{n} \sum^n_{i=1} X_i$$. Then as $n \to \infty$, the following holds.
>
> $$
\sqrt{n}(\bar{X}_n - \mu) \overset{d}{\to} \mathcal{N}(0,\sigma^2)
> $$
>
> The same holds for multidimensional random variables with finite covariance.

*proof*. Normalize the random variables as $Y_i \equiv (X_i - \mu)/\sigma$. Now define a new random variable

$$
Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} = \sum^n_{i=1} \frac{1}{\sqrt{n}} Y_i
$$

Since $Y_i$ are i.i.d. distributed over a standard Gaussian distribution, the characteristic functions of each $Y_i$, denoted $\phi_{Y_i}(t)$, are identical. From Taylor's theorem we expand

$$
\phi_{Y_i}(t) = 1 - \frac{t^2}{2} + o\left({t^2}\right)
$$

Therefore, the characteristic function of $Z_n$ becomes

$$
\phi_{Z_n}(t) = \prod^n_{i=1} \phi_{Y_i}\left(\frac{t}{\sqrt{n}}\right) = \left( 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right) \right)^n \to e^{-\frac{1}{2}t^2}, \quad n \to \infty
$$

As the limit $n \to \infty$ is taken, all the higher orders vanish. Therefore, the sequence of characteristic functions $\phi_{Z_n}$ converges pointwise to the characteristic function of a standard Gaussian distribution. From Lévy's continuity theorem, the random variables $Z_n \overset{d}{\to} \mathcal{N}(0,1)$. The proof is complete.

>**Theorem** (Markov's Inequality) If $X$ is a nonnegative random variable and $a > 0$, then
>
>$$
\mathrm{Pr}[X \ge a] \le \frac{\expectation(X)}{a}
>$$

*proof*. Use $$\mathbf{1}\{X \ge \varepsilon\} \le \frac{1}{\varepsilon}X$$, then we conclude

$$
\mathrm{Pr}[X \ge \varepsilon] = \expectation[\mathbf{1}\{X \ge \varepsilon\}] \le \expectation\left[\frac{1}{\varepsilon}X \right]
$$

>**Theorem** (Chebyshev’s Inequality) Let $X_1,\dots,X_n$ be independent random variables with mean $\mu$ and variance $\sigma^2$. Let $\bar{X} = \frac{1}{N}( X_1 + \cdots + X_N )$. Then the following holds.
>
>$$
\mathrm{Pr}[|\bar{X}-\mu| \ge \varepsilon] \le \frac{\sigma^2}{N \varepsilon^2}
>$$

_proof_. Use Markov's inequality.

$$
\mathrm{Pr}[|\bar{X}-\mu| \ge \varepsilon] = \mathrm{Pr}[(\bar{X}-\mu)^2 \ge \varepsilon^2] \le \frac{\sigma^2}{N \varepsilon^2}
$$

>**Definition** (sub-Gaussian) A random variable $X$ with finite mean $\mu = \mathbb{E}[X]$ is called sub-Gaussian with variance proxy $\sigma$ if
>
>$$
\mathbb{E}[e^{\lambda(X-\mu)}] \le e^{\sigma^2 \lambda^2 / 2} \quad \text{for } \forall \lambda \in \mathbb{R}
>$$

Inforamally, sub-Gaussian distributions have tails upper bounded by Gaussian distributions, hence the *sub-* prefix. The following corollary directly shows such tail bound.

>**Corollary** If $X$ is sub-Gaussian with variance proxy $\sigma^2$, then
>
>$$
\mathrm{Pr} [|X-\mu| \ge t] \le 2 \exp\left( -\frac{t^2}{2\sigma^2} \right)
>$$

*proof*. Fix $t > 0$. For any $\lambda > 0$,

$$
\begin{aligned}
\mathrm{Pr}[X - \mu \ge t] &= \mathrm{Pr}\left[e^{\lambda(X-\mu)} \ge e^{\lambda t}\right] \\
                           &\le e^{-\lambda t} \expectation\left[ e^{\lambda(X-\mu)} \right], \quad \text{(by Markov's inequality)} \\
                           &\le e^{-\lambda t} e^{\sigma^2 \lambda^2 / 2}, \quad \text{(by definition of sub-Gaussian)} \\
                           &= \exp(-\lambda t + \sigma^2 \lambda^2 / 2)
\end{aligned}
$$

Since the inequality holds for any $\lambda > 0$, we minimize the right hand side with respect to $\lambda$. Therefore, we have the tight bound

$$
\mathrm{Pr}[X - \mu \ge t] \le \exp\left(-\frac{t^2}{2\sigma^2}\right)
$$

Similarly, we can bound the opposite tail as

$$
\mathrm{Pr}[X - \mu \le -t] \le \exp\left(-\frac{t^2}{2\sigma^2}\right)
$$

Therefore, we have the tail bound on both sides as stated above. The proof is complete.

>**Theorem** If $X_1,\dots,X_n$ are independent sub-Gaussian random variables with variance proxy $\sigma_1^2,\dots,\sigma_n^2$ respectively, then $Z = X_1 + \cdots + X_n$ is sub-Gaussian with variance proxy $\sum_i \sigma_i^2$.

*proof*. Use the independence of $X_i$. Then we can separate

$$
\expectation\left[\exp\{\lambda(Z - \expectation Z)\}\right] = \expectation\left[ \prod^n_{i=1} \exp\{ \lambda(X_i - \expectation X_i) \} \right]
$$

The rest of the proof is clear.

>**Theorem** (Hoeffding's Lemma) If $X$ is a random variable such that $A \le X \le B$ almost surely, then
>
>$$
\mathbb{E} [e^{\lambda(X-\mathbb{E}X)}] \le e^{\lambda^2(B-A)^2/8}
>$$

First, we can shift each variable such that $\expectation X = 0$, and $A \le 0 \le B$. We use the fact that $e^{\lambda x}$ is convex. For all $x \in [A,B]$,

$$
e^{\lambda x} \le \frac{B-x}{B-A}e^{\lambda A} + \frac{x-A}{B-A} e^{\lambda B}
$$

Therefore,

$$
\expectation\left[e^{\lambda X}\right] \le \frac{B}{B-A} e^{\lambda A} + \frac{-A}{B-A}e^{\lambda B} = e^{L(\lambda(B-A))}
$$

where $L(h) = \frac{hA}{B-A} + \log\left(1 + \frac{A - e^hA}{B-A}\right)$. Using $L(0) = L'(0) = 0$, and bounding $L^"(h) \le 1/4$ (which can be done easily), we have the following bound

$$
L(h) \le \frac{1}{8}h^2
$$

Thus, $\expectation\left[e^{\lambda X}\right] \le \exp\left(\lambda^2(B-A)^2/8\right)$. The proof is complete.

We can see that, if a random variable $X$ is bounded $A \le X \le B$ almost surely, then it is sub-Gaussian with variance proxy $(B-A)/2$.

> **Theorem** (Hoeffding’s Inequality) Let $X_1, \dots, X_n$ be independent random variables. Suppose $a_i < X_i < b_i$ almost surely. Let $\mu = \mathop{\mathbb{E}}\left[\frac{1}{n}\sum_i X_i \right]$, then for $\forall \varepsilon > 0$, the following holds.
>
>$$
\mathrm{Pr} \left[ \left|\frac{1}{n} \sum^n_{i=1} X_i - \mu\right| \le \varepsilon \right] \ge 1 - 2 \exp\left( -\frac{2n^2\varepsilon^2}{\sum_i (b_i - a_i)^2} \right)
>$$

*proof*. From Hoeffding's lemma, the variables $X_i$ are sub-Gaussian with variance proxy $(b_i - a_i)^2/4$. Therefore, the random variable $\bar{X} = \frac{1}{n} \sum^n_{i=1} X_i$ is sub-Gaussian with variance proxy $ \sum^n_{i=1} (b_i - a_i)^2 / {2n^2} $. From the corollary of tail bounds of sub-Gaussian variables above, the proof is complete.

Hoeffding's inequality takes the form of a PAC-bound, and can be used to transform bounds on the expectation of a variable into a PAC-bound on the empirical samples mean.

> **Theorem** (Azuma-Hoeffding) Let $X_1,\dots,X_N \in \R$ be a martingale difference sequence, i.e.,
>
> $$
\expectation\left[ X_i | X_{i-1},\dots,X_i \right] = 0 \quad \text{for } i = 1,\dots,N,
> $$
>
> such that
$$|X_i| \le c_i < \infty$$
for $i = 1,\dots,N$. Let $\bar{X} = \frac{1}{n} \sum^n_{i=1} X_i$. Then for any $\varepsilon > 0$,
>
> $$
\mathrm{Pr}\left[ \bar{X} \ge \varepsilon \right] \le \exp \left( -\frac{N^2 \varepsilon^2}{2(c_1^2 + \cdots + c_N^2)} \right)
> $$

*proof*. The proof is almost identical to Hoeffding's inequality.

> **Theorem** (McDiarmid’s Inequality) Suppose $f$ satisfy the *bounded difference condition*, i.e.,
>
>$$
\forall x_1,\dots,x_n\quad \forall i \quad |f(x_1,\dots,x_i,\dots,x_n) - f(x_1\dots, x_i',\dots,x_n)| \le c_i
>$$
>
>Let $X_1,\dots,X_n$ be independent random variables, then,
>
>$$
\mathrm{Pr}[f(X_1,\dots,X_n)-\mathbb{E}f(X_1,\dots,X_n) \ge t] \le \exp\left(-\frac{2t^2}{\sum_i c_i^2}\right)
>$$
>
> McDiarmid generalizes Hoeffding with $f = \frac{1}{N}\sum^N_{i=1} Z_i$ and $c = 1$.

*proof*. We show a one-sided tail bound. Define

$$
V_i \equiv \expectation\left[ f(Z_1, \dots, Z_N) | Z_1, \dots, Z_i \right] - \expectation\left[ f(Z_1,\dots,Z_N) | Z_1, \dots, Z_{i-1} \right]
$$

where for $i=1$, we have

$$
\expectation\left[ f(Z_1,\dots,Z_N) | \emptyset \right] = \expectation\left[f(Z_1,\dots,Z_N)\right]
$$

By definition,

$$
f(Z_1, \dots, Z_N) - \expectation\left[ f(Z_1,\dots,Z_N) \right] = \sum^N_{i=1} V_i
$$

By the law of iterated expectations,

$$
\expectation\left[ V_i | Z_1, \dots, Z_{i-1} \right] = 0
$$

Therefore, $V_1, \dots, V_N$ is a martingale difference sequence. By the bounded difference property,

$$
V_i = \expectation\left[ \expectation_{Z_i'} \left[ f(\dots,Z_i,\dots) - f(\dots,Z_i',\dots) \right] \middle| Z_1, \dots, Z_i \right]
$$

and therefore

$$
|V_i| \le \expectation\left[ \expectation_{Z_i'} \left[ |f(\dots,Z_i,\dots) - f(\dots,Z_i',\dots)| \right] \middle| Z_1, \dots, Z_i \right] \le \expectation\left[ \expectation_{Z_i'}[c_i] \middle| Z_1,\dots,Z_i \right] = c_i
$$

Finally, we use Hoeffding with the martingale difference sequence $V_1,\dots,V_N$ to conclude the statement.

Below is a much stronger version of McDiarmid, which is stated in [3], theorem 3.18.

> **Theorem** Define the Gaussian tail bounds in terms of one-sided differences.
>
$$
D_i^-f(x) \equiv f - \inf_z f(x_1,\dots,x_{i-1},z,x_{i+1},\dots,x_n) \ge 0
$$
>
>$$
D_i^+f(x) \equiv \sup_z f(x_1,\dots,x_{i-1},z,x_{i+1},\dots,x_n)-f \ge 0
>$$
>
>Also denote
>
>$$
d^+ = \sup_{x}\left[ \sum^n_{i=1} |D_i^+f(x)|^2 \right]
>$$
>
>$$
d^- = \sup_{x}\left[ \sum^n_{i=1} |D_i^-f(x)|^2 \right]
>$$
>
>Then the following inequalities hold.
>
>$$
\mathrm{Pr}[f(x) - \mathbb{E}f(x) \ge t] \le \exp\left(-\frac{t^2}{4d^-}\right)
>$$
>
>$$
\mathrm{Pr}[f(x) - \mathbb{E}f(x) \le -t] \le \exp\left(-\frac{t^2}{4d^+}\right)
>$$

## References

[1] Ma, Tengyu. 2022. Lecture notes for CS229M (Machine Learning Theory) \
[2] Bach, Francis. 2024. Learning Theory from First Principles. London, England: MIT Press. \
[3] van Handel, R. 2014. Probability in High Dimension. APC 550 Lecture Notes, Princeton University
