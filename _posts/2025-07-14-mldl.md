---
title: A Bird-Eye View In ML & DL Theory
date: 2025-07-14 20:00:00 +0800
description: Categorization and current challenges in classical ML / deep learning theory. May be outdated.
categories: [machine-learning-deep-learning, classic-ml-theory]
tags: [lecture-notes]
math: true
toc: false
---
## Classical ML Theory

Roughly speaking, there are three challenges in classical machine learning theory.
- (Approximation theory / expressivity / representation power) Dealing with bounding $L(\theta^*)$ - how good can a hypothesis class approximate the ground truth?
- (Generalization theory) Trying to bound the difference between the learned hypothesis (from data) and the optimal hypothesis, i.e., $$
L(\hat{\theta}) - L(\theta^*) \le L(\hat{\theta}) - \hat{L}(\hat{\theta}) + |L(\theta^*) - \hat{L}(\theta)|
$$. We call $$L(\hat{\theta})-\hat{L}(\hat{\theta})$$ the _generalization error_ (difference between training loss and test loss, based on the empirically learned hypothesis). We roughly bound the generalization error by $\sqrt{\text{complexity}/n}$, i.e., the simpler a model is and the more data points we have, the better it generalizes.
    - We can see that there is a tradeoff between the two aspects of classical ML theory (which is related to the _bias-variance decomposition_).
    - In generalization theory, we can also introduce the regularization term $\sim \lambda R(\theta)$, then make the statistical claim that if $$\hat{\theta}_\lambda$$ is the global minimum of $$\hat{L}_\textrm{reg}$$, then $$L(\hat{\theta}_\lambda) - L(\theta^*)$$ is bounded above.
- (Optimization) Numerically finding $$\hat{\theta} = \mathrm{argmin}_{\theta \in \Theta}\,\hat{L}(\theta)$$ (which is basically a seperate question). Methods include convex optimization, SGD, etc. 

## Deep Learning Theory

When we introduce deep neural nets, many aspects of classical ML we discussed above behaves differently.
- One reason is due to nonlinearity &rarr; the optimziation process becomes nonconvex.
- Empirically, overparametrized models perform better (even when the number of parameter exceeds the number of data points, or even when you reach zero training error)
- (Approximation theory) Things don't change as much, as we can take the training loss very low even with few neurons.
- (Regularization) In practice, only weak regularization is used (typically L2 norm with small constants). Also, the following statements hold.
    - $$\hat{L}_\mathrm{reg}(\theta)$$ doesn't have a unique global minimizer. Instead, it have multiple approximate minimizers.
    - It is not true that all of these approximate global minimizers perform equally well on the test set, which means that you cannot bound the generalization error solely by finding the global minimizer.
    - Different optimizers find different minimizers with different performance. Therefore, optimizing is not only about finding _any_ global minimum, but finding a **generalizable** global minimum. We have to find 'why' some optimizers find a 'better' $\hat{\theta}$ (that has some special property, e.g., low complexity).

## Current Developed Theory & Challenges

_Disclamer: This is based on lecture videos from 2022, and thus the information here is most likely outdated_

- **Task #1** (Optimization) Prove that some optimizer converges to an approximate global/local minimum of $\hat{L}(\theta)$.
    - Without overparametrization, we can prove in special cases (e.g., matrix factorization, linearized NNs, ..) that GD/SGD can converge to global min.
    - For overparametrized neural networks, for _special hyperparameters_ (overparameterization, special learning rate, special initialization, batch size, sometimes only for GD, …) we can prove convergence (**neural tangent kernel** approach).
- **Task #2** (Implicit regularization) In addition to #1, show that $\hat{\theta}$ also has low complexity, e.g., $R(\theta) \le C$ for some complexity measure $R$ (this task depends on hyperparameters such as learning rate, batch size, etc.)
    - For special/simplified models and optimizers (e.g., linear regression, logistic regression, matrix factorization problems, linear NNs), this is obtainable.
    - Focusing on the hyperparameters of each optimizers: GD, SGD (noise covariance, noise scale), dropout, learning rate, batch size, momentum, etc. → everything that you change in the optimizer can have some implicit effect on the performance!
- **Task #3** (Generalization bound) Prove that $\forall \theta$ such that $R(\theta) \le C$ and $\hat{L}(\theta) \simeq 0$, the test error is also small
    - (Bartlett _et al._, 2017) Roughly, the product of the spectral norms for each weights can be viewed as a complexity bound (however, this result is not precise enough).
    - Find a more fine-grained measure of complexity for generalization error bounds!

## References
[1] Ma, Tengyu. 2022. Lecture notes for CS229M (Machine Learning Theory)