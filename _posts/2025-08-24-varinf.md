---
title: Variational Inference, ELBO, and VAEs
date: 2025-08-24 05:50 +0800
description: Introduction to variational inference, minimization of ELBO, and variational autoencoders.
categories: [machine-learning-deep-learning, unsupervised-learning]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## Variational Inference

Consider a probability distribution over a variable $x$ and latent variable $z$, where we know the joint distribution $p(x,z)$. Now consider a situation where $x$ is observed, and is represented as a set of data points $\mathcal{D}$. We want to find the posterior distribution
$$p(z\,|\,\mathcal{D})$$
. From Bayes's rule,

$$
p(z|x=\mathcal{D}) = \frac{p(x=\mathcal{D}\,|\,z)\cdot p(z)}{p(x = \mathcal{D})}
$$

We consider problems where computing the marginal is an intractable problem. Instead of directly using
$$p(z\,|\,\mathcal{D})$$
, we approximate the distribution using $q(z)$, which is an element of the set of some parameterized well-known set of distributions (e.g. Gaussians). To find the closest possible distribution $q(z)$, we minimize the KL divergence

$$
q^* (z) = \argmin_{q \in Q} \left( D_{KL} \left[ \, q(z) \, \middle| \, p(z \, |\, \mathcal{D}) \, \right] \right)
$$

Now rearrange KL divergence

$$
\begin{aligned}
D_{KL} \left[ \, q(z) \, \middle| \, p(z \, |\, \mathcal{D}) \, \right] &= \int q(z) \log \left( \frac{q(z)}{p(z\,|\,\mathcal{D})} \right) \, dz \\
&= \int q(z) \log \left( \frac{q(z)\cdot p(\mathcal{D})}{p(z,\mathcal{D})} \right) \, dz \\
&= \int q(z) \log \left( \frac{q(z)}{p(z,\mathcal{D})} \right) \, dz + \int q(z) \log \left( p(\mathcal{D}) \right) \, dz \\
&= - \expectation_{z \sim q(z)} \left[ \log \left( \frac{p(z,\mathcal{D})}{q(z)} \right) \right] + \log\left(p(\mathcal{D})\right)
\end{aligned}
$$

We call the first quantity the ELBO (evidence lower bound) and the second quantity as the _evidence_ (fixed quantity). Therefore, we maximize the ELBO instead of minimizing the KL divergence.

## Variational Autoencoders

Autoencoders consists of an encoder, which maps variables to latent variables, i.e., $x \to z$, and decoders, which perform the opposite. We denote the encoder as
$$q_\phi(z\,|\,x)$$
, which induces a probability distribution on $z$, and the decoder as
$$p_\theta(x \, | \, z)$$
. Now, given some data from a probability distribution $p(x)$ (which is impossible to know exactly) we want to optimize the encoder
$$q_\phi(z\,|\,x) \sim p_\theta(z\,|\,x)$$
. We assume that the family $Q$ consist of Gaussian distributions, for convenience in calculation the binary cross-entropy and the KL divergence. We split the ELBO into two terms using Bayes's rule.

$$
\begin{aligned}
\mathrm{ELBO} &= \int dz \, q_\phi(z\,|\,x) \log p_\theta(x \, | \, z) - \int dz \, q_\phi(z\,|\,x) \log \frac{q_\phi(z\,|\,x)}{p(z)} \\ &= \expectation_{z \sim q_\phi}\left[ \log p_\theta(x\,|\,z) \right] - D_{KL} \left[q_\phi(z\,|\,x) \middle\| p(z) \right]
\end{aligned}
$$

Both of which can be exactly calculated for Gaussian distributions.

An issue that occurs is - how can we optimize the variables $\theta$, $\phi$ using stochastic gradient descent? We solve this issue using the _reparametrization trick_, where the latent variables are sampled using $z_j = \mu_j + \varepsilon \sigma_j$ where $\varepsilon \sim \mathcal{N}(0,1)$. Then the parameters are deterministically evaluated using an encoder neural network

$$
(\mu, \log \sigma) = \mathrm{EncoderNeuralNet}_\phi(x)
$$

After evaluation, the stochastic latent variables $z_j$ are sampled from a Gaussian distribution, and finally $z_j$ is passed through the decoder neural network to achieve the final result

$$
\hat{x} = \mathrm{DecoderNeuralNet}_\theta(x)
$$

Now let's look back at the ELBO. The first term is approximated using Monte-Carlo approximation over the dataset $x^{(i)}$, i.e.,

$$
\expectation_{z \sim q_\phi}\left[ \log p_\theta(x\,|\,z) \right] \approx \frac{1}{L} \sum^L_{i=1} \log p_\theta\left(\hat{x}^{(i)} \middle| z^{(i)}\right)
$$

If we assume the input is sample from a binary Bernoulli distribution, the first term is the binary cross-entropy loss between the input and computed output, i.e.,

$$
\log p_\theta\left(\hat{x}^{(i)} \middle| z^{(i)}\right) = \sum_j \left[ x_j^{(i)} \log \hat{x}_j^{(i)} + \left(1-x_j^{(i)}\right) \log \left(1-\hat{x}_j^{(i)}\right)  \right]
$$

Assume that the prior $p(z) = \mathcal{N}(0,1)$. Then the second term is the KL divergence between two Gaussian distributions.

$$
D_{KL} \left[q_\phi(z\,|\,x) \middle\| p(z) \right] = - \log \sigma - \frac{1}{2} + \frac{1}{2}(\mu^2 + \sigma^2)
$$

Therefore, we constructed the ELBO with parameters $\theta$, $\phi$, that can succesfully be trained using SGD. Then the probability distribution $p_\theta(x,z)$ can be used as a generative model.

## References

[1] Kingma, D. P., & Welling, M. (2019). An Introduction to Variational Autoencoders. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1906.02691
