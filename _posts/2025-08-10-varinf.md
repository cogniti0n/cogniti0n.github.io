---
title: Variational Inference, ELBO, and VAEs
date: 2025-08-10 05:50 +0800
description: Introduction to variational inference, minimization of ELBO, and variational autoencoders.
categories: [machine-learning-deep-learning, unsupervised-learning]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## Variational Inference

Consider a probability distribution over a variable $x$ and latent variable $z$, where we know the joint distribution $p(x,z)$. Now consider a situation where $x$ is observed, and is represented as a set of data points $\mathcal{D}$. We want to find the posterior distribution 
$$p(z\,|\,\mathcal{D})$$
. From Bayes's rule,

$$
p(z|x=\mathcal{D}) = \frac{p(x=\mathcal{D}\,|\,z)\cdot p(z)}{p(x = \mathcal{D})}
$$

We consider problems where computing the marginal is an intractable problem. Instead of directly using 
$$p(z\,|\,\mathcal{D})$$
, we approximate the distribution using $q(z)$, which is an element of the set of some parameterized well-known set of distributions (e.g. Gaussians). To find the closest possible distribution $q(z)$, we minimize the KL divergence

$$
q^* (z) = \argmin_{q \in Q} \left( D_{KL} \left[ \, q(z) \, \middle| \, p(z \, |\, \mathcal{D}) \, \right] \right)
$$

Now rearrange KL divergence

$$
\begin{aligned}
D_{KL} \left[ \, q(z) \, \middle| \, p(z \, |\, \mathcal{D}) \, \right] &= \int q(z) \log \left( \frac{q(z)}{p(z\,|\,\mathcal{D})} \right) \, dz \\
&= \int q(z) \log \left( \frac{q(z)\cdot p(\mathcal{D})}{p(z,\mathcal{D})} \right) \, dz \\
&= \int q(z) \log \left( \frac{q(z)}{p(z,\mathcal{D})} \right) \, dz + \int q(z) \log \left( p(\mathcal{D}) \right) \, dz \\
&= - \expectation_{z \sim q(z)} \left[ \log \left( \frac{p(z,\mathcal{D})}{q(z)} \right) \right] + \log\left(p(\mathcal{D})\right)
\end{aligned}
$$

We call the first quantity the ELBO (evidence lower bound) and the second quantity as the _evidence_ (fixed quantity). Therefore, we maximize the ELBO instead of minimizing the KL divergence.

## Variational Autoencoders

Autoencoders consists of an encoder, which maps variables to latent variables, i.e., $x \to z$, and decoders, which perform the opposite. We denote the encoder as 
$$q_\phi(z\,|\,x)$$
, which induces a probability distribution on $z$, and the decoder as 
$$p_\theta(x \, | \, z)$$
. Now, given some data from a probability distribution $p(x)$ (which is impossible to know exactly) we want to optimize the encoder 
$$q_\phi(z\,|\,x) \sim p_\theta(z\,|\,x)$$
, where now we assume that the family $Q$ is the family of Gaussian distributions.

Therefore, given a dataset, our goal is to optimize the variables $(\mu_i,\sigma_i)$ such that the ELBO is maximized. Rewriting the ELBO, we find that



This is performed using the _reparametrization trick_, where the latent variables are sampled using $z_j = \mu_j + \varepsilon \sigma_j$ where $\varepsilon \sim \mathcal{N}(0,1)$.