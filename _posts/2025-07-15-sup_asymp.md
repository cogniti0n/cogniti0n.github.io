---
title: Supervised Learning, Asymptotics
date: 2025-07-15 22:00:00 +0800
description: Part 2 of notes on C229M. Supervised learning setup in classical ML and asymptotic behavior.
categories: [machine-learning-deep-learning, classic-ml-theory]
tags: [lecture-notes]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
$$

We start from a training dataset $$\{x_i,y_i\}$$, a loss function $l: Y\times Y \to \mathbb{R}$, and a hypothesis $h_\theta: X \to Y$ parametrized by $\theta\in\Theta$. We measure the empirical loss of a data as $l(h_\theta(x),y) \equiv l((x,y),\theta)$. The population loss, or expected loss is

$$
L(h)=\mathop{\mathbb{E}}_{(x,y)\sim p} \left[l\left(h(x),y\right)\right] = \expectation_{(x,y)\sim p} \left[l\left((x,y), \theta\right)\right]
$$

Then, the **excess risk** of a hypothesis is

$$
E(h)\equiv L(h) - \inf_{g \in \mathcal{H}} L(g)
$$

which is how ‘bad’ the hypothesis is compared to the ‘best case'. The empirical risk is measured as

$$
\hat{L}(h)=\frac{1}{n}\sum^n_{i=1}l\left(\left(x_i,y_i\right),\theta\right)
$$

and the empirical risk minimizer (ERM) is $\hat{\theta} \equiv \argmin_{\theta \in \Theta} \hat{L}(h_\theta)$. We denote the the minimizer of the population loss as $\theta^* = \argmin_{\theta \in \Theta} L(h_\theta)$.

Our goal is to use the data we have, to try and find the underlying probability distribution, which in turn is achieved by finding the expectation loss minimizer. However, since we cannot find neither the 'true' distribution nor the minimizer of the population loss, we can only go so far as the empirical risk minimizer. Therefore, we want to see ‘how good’ of an estimation the ERM is, compared to the theoretical value.

Asymptotically, we have the following theorem.

>**Theorem** Assume $\hat{\theta} \overset{p}{\to} \theta^* $ and $ H \equiv \nabla^2 L(\theta^*)$ is full-rank, and other regularity conditions hold (we won’t get into much details). Then, the following holds.
>
> (1)
>$$
\sqrt{n}(\hat{\theta} - \theta^*) \overset{d}{\to} \mathcal{N}\left(0,H^{-1}\mathrm{cov}\left[\nabla l((x,y),\theta^*)\right]H^{-1}\right)
>$$
>
> (2)
>$$
n(L(\hat{\theta}) - L(\theta^*)) \overset{d}{\to} \frac{1}{2} ||S||^2_2
>$$, where
>$$
S \sim \mathcal{N}\left(0,H^{-1/2}\mathrm{cov} \left[\nabla l((x,y),\theta^*) \right] H^{-1/2}\right)
>$$

_An outline of the proof_. Taylor expand $\nabla\hat{L}(\theta)$ around $\hat{\theta}$ and plug in $\theta^*$. Use the fact that $\nabla\hat{L}(\hat{\theta})=0$.

Even if we skimmed over the actual proof, here is a useful lemma used in the proof.

> **Lemma** Assume $Z \sim \mathcal{N}(0,\Sigma)$. Then, $AZ \sim \mathcal{N}(0, A\Sigma A^T)$.

A more important result is the corollary below. For now, I won't state the full proof, but the theorem below tells us that, with enough data, we can approximate the population loss minimizer using the ERM, and the 'ground truth' as the population loss minimizer.

>**Corollary** In addition to the theorem above, there exist probabilistic models parametrized by $\theta$, denoted as
>$$p(y|x;\theta)$$, and $\exists \theta_* $ such that $y_i | x_i \sim p(y|x;\theta_* ) $ (represents the ‘ground truth’). Then if we set the loss function to the negative log-likelihood function, i.e.,
>
>$$
l\left(\left(x_i, y_i \right) , \theta\right) = -\log p\left(y_i | x_i ; \theta\right)
>$$
>
>then the following holds.
>
> - $\theta_* = \theta^*$ (the population risk minimizer of $n \to \infty$ case is the same as the ground truth)
> - $\expectation [\nabla l((x,y),\theta^*)] = 0$
> - $\mathrm{cov}\left[\nabla l ((x,y),\theta^* )\right] = \nabla^2 L(\theta^* )$
> - $\sqrt{n}(\hat{\theta} - \theta^* ) \overset{d}{\to} \mathcal{N} (0, \nabla^2 L(\theta^* )^{-1})$ (the difference scales with $1/\sqrt{n}$, therefore, as the number of data points reach $\infty$, the empirical risk minimizer is equal to the population risk minimizer)
> - $n(L(\hat{\theta}) - L(\theta^*)) \overset{d}{\to} \frac{1}{2}\chi^2(p)$ where $p$ is the dimension of $\theta$ (number of parameters)

To conclude, the difference between the empirical risk minimizer and the population risk minimizer scales with $1/n$ as $n \to \infty$.

$$
\mathbb{E}[L(\hat{\theta})-L(\theta^*)]\to\frac{p}{2n}+O\left(\frac{1}{n}\right)
$$

where $O$ only contains dependence of $n$. 

**Issue** We want to see dependence regarding other parameters; in reality, the generalization performance relies heavily on other parameters, such as the dimension of $\theta$. Therefore, we want to know what is hidden in that $O(\cdot)$ term. Hereinafter, we denote $O(x)$ as a placeholder for some $f(x)$ such that
$$
\forall x \in \mathbb{R},\,|f(x)|\le C x
$$
(i.e., only hiding absolute constants).

## References

[1] Ma, Tengyu. 2022. Lecture notes for CS229M (Machine Learning Theory)