---
title: SVD As a Low-Rank Approximation
date: 2025-07-14 11:33:00 +0800
description: Truncated singular-value decompositon is the best rank-$r$ decomposition.
categories: [math-statistics]
math: true
toc: false
---
## Singular Value Decomposition

Take a real-valued $m\times n$ matrix $A$. Then its singular value decomposition is the following decomposition.

$$
A = U\Sigma V^T
$$

where $U$ and $V$ are each $m\times m$ and $n \times n$ orthogonal matrices, i.e., $U^T U= I$ and $V^T V = I$, and $\Sigma_{ij} = \lambda_i \delta_{ij}$ where $\lambda_1 \ge \cdots \ge \lambda_n \ge 0$. $\lambda_{ij}$ are the *singular values* of $A$. The singular values are equal to the square roots of the eigenvalues of $A^TA$ ($AA^T$), with eigenvectors $V$ ($U$). 

## Thin SVD, Truncated SVD

Hereinafter, $m > n$ without loss of generality. Then, we can perform *thin SVD*, where we truncate the last $m-n$ rows of $\Sigma$, which results in truncating the last $m-n$ columns of $U$. Now, $U$ is an $m \times n$ left-orthogonal matrix, i.e., $U^T U = I_{n\times n}$.

We can take this process a step further by truncating the singular values, which results in both $U$ and $V$ being truncated. If we leave $r$ largest singular values, then $U$ and $V$ become $m \times r$ and $r \times n$ respectively. Now consider

$$
\tilde{A}=\sum^r_{i=1} u_i\lambda_i v_i^T
$$

where $u_i$ and $v_i$ is the $i$-th column of $U$ and $V$, respectively. This computation is called *truncated SVD*. The resulting matrix is a rank-$r$ matrix $\tilde{A}$.

## Eckart-Young Theorem

> **Theorem** (Eckart-Young) $\tilde{A}$ is the best rank-$r$ approximation of $A$, in terms of both spectral norm (largest singular value) and Frobenius norm. We can easily see that the spectral norm is
$$
||\tilde{A}-A||_2 = \lambda_{r+1}
$$ 
> and Frobenius norm is
$$
||\tilde{A} - A||_F = \sqrt{\sum_{i=r+1}^n \lambda_i^2}
$$

_proof_. We first prove the theorem for the spectral norm. Assume that $\exists B \in M_{m \times n}(\mathbb{R})$ with $\text{rank}(B) = r$ such that
$$||A-B||_2 < ||A-\tilde{A}||_2 = \lambda_{r+1}$$. Take any $w \in \mathrm{null}(B)$. Then the following inequality holds.

$$
||(A-B)w||_2 = ||Aw||_2 < \lambda_{r+1} ||w||_2 \tag{1}
$$

Now take any $w \in \mathrm{span}(v_1, \dots, v_{r+1})$. Then there exist $c_i,\dots,c_{r+1}$ such that

$$
||Aw||_2 = \left \Vert \sum^{r+1}_{i=1} c_i A v_i \right \Vert_2 = \sum^{r+1}_{i=1} |c_i| \lambda_i \ge \lambda_{r+1} ||w||_2 \tag{2}
$$

We know from the rank-nullity theorem that $\mathrm{nullity}(B) = n-r$. Therefore, $\exists w \ne 0$ such that $w \in \mathrm{null}(B) \cap \mathrm{span}(v_1, \dots, v_{r+1})$. However, such a $w$ has to satisfy inequalities (1) and (2), which is contradictory.

Now we prove the theorem for the Frobenius norm. We first prove a lemma.

> **Lemma** (Weyl) Let $A,B \in M_{m \times n}(\mathbb{R})$, then $$ \lambda_{i+j-1}(A+B) \le \lambda_i(A) + \lambda_j(B) $$

_proof_. First by triangular inequality of the spectral norm, $\lambda_1(A+B) \le \lambda_1(A) + \lambda_2(B)$. Also, for any matrix $A$, take $\tilde{A}_i$ to be the rank-$i$ approximation of $A$ using truncated SVD. Then for any $i,j \ge 1$, we see that

$$
\begin{aligned}
\lambda_i(A) + \lambda_j(B) &= \lambda_1(A - \tilde{A}_{i-1}) + \lambda_1(B - \tilde{B}_{j-1}) \\
                            &\ge \lambda_1(A + B - \tilde{A}_{i-1} - \tilde{B}_{j-1}) \\
                            &\ge \lambda_1(A + B - \widetilde{(A+B)}_{i+j-2}) \\
                            &= \lambda_{i+j-1}(A+B)
\end{aligned}
$$

We now prove the Eckart-Young theorem for Frobenius norm. Take a rank-$r$ matrix $B \in M_{m\times n}(\mathbb{R})$. From the lemma above, we can directly see that

$$
\lambda_{i+r}(A) \le \lambda_i(A-B) + \lambda_{r+1}(B) = \lambda_i(A)
$$

where the last equality holds due to $\mathrm{rank}(B) = r$. It follows that

$$
||A-\tilde{A}||_F^2 = \sum^n_{i=r+1} \lambda_i^2 = \sum^{n-r}_{i=1} \lambda^2_{i+r}(A) \le \sum^{n-r}_{i=1} \lambda_i^2(A-B) \le ||A-B||_F^2
$$

The proof is complete.

## References
[1] Lee, S.-S. B. 2022. Lecture notes for Tensor Networks, Fall 2022