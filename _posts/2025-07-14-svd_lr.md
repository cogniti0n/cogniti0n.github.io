---
title: SVD As a Low-Rank Approximation
date: 2025-07-14 11:33:00 +0800
#description: qwer
categories: [math,linear-algebra]
math: true
---
## Singular Value Decomposition

Take a real-valued $m\times n$ matrix $A$. Then its singular value decomposition is the following decomposition.

$$
A = U\Sigma V^T
$$

where $U$ and $V$ are each $m\times m$ and $n \times n$ orthogonal matrices, i.e., $U^T U= I$ and $V^T V = I$, and $\Sigma_{ij} = \lambda_i \delta_{ij}$ where $\lambda_1 \ge \cdots \ge \lambda_n \ge 0$. $\lambda_{ij}$ are the *singular values* of $A$. The singular values are equal to the square roots of the eigenvalues of $A^TA$ ($AA^T$), with eigenvectors $V$ ($U$). 

Hereinafter, $m > n$ without loss of generality. Then, we can perform *thin SVD*, where we truncate the last $m-n$ rows of $\Sigma$, which results in truncating the last $m-n$ columns of $U$. Now, $U$ is an $m \times n$ left-orthogonal matrix, i.e., $U^T U = I_{n\times n}$.

## Thin SVD, Truncated SVD

We can take this process a step further by truncating the singular values, which results in both $U$ and $V$ being truncated. If we leave $r$ largest singular values, then $U$ and $V$ become $m \times r$ and $r \times n$ respectively. Now consider

$$
\tilde{A}=\sum^r_{i=1} u_i\lambda_i v_i^T
$$

where $u_i$ and $v_i$ is the $i$-th column of $U$ and $V$, respectively. This computation is called *truncated SVD*. The resulting matrix is a rank-$r$ matrix $\tilde{A}$.

## Eckart-Young Theorem

> **Theorem**. (Eckart-Young) $\tilde{A}$ is the best rank-$r$ approximation of $A$, in terms of both spectral norm (largest singular value) and Frobenius norm. We can easily see that the spectral norm $||\tilde{A}-A||_2 = \lambda_{r+1}$ and Frobenius norm $||\tilde{A} - A||_F = \sqrt{\sum_{i=r+1}^N \lambda_i^2}$.

_proof_. We first prove the theorem for the spectral norm. We need to show that, for any rank-$r$ $B$, $||A - B||_2 \ge \lambda_{r+1}$. Since $B$ is rank-$r$, there must be a nontrivial linear combination of the first $r+1$ columns of $V$ (denoted $w$) such that $Bw = 0$. Without loss of generality, rescale $w$ so that $||w||_2 = 1$ and denote $w = \gamma_1 v_1 + \cdots + \gamma_{r+1} v_{r+1}$. Therefore,

$$
||A - B||_2^2 \ge ||(A-B)w||^2_2 = ||Aw||_2^2 = \gamma_1^2 \lambda_1^2 + \cdots + \gamma_{r+1}^2 \lambda_{r+1}^2 \ge \lambda_{r+1}^2
$$

Now we prove the theorem for the Frobenius norm.

## References
[1] S.-S. B. Lee, Lecture notes for "Tensor Networks 2022-2"