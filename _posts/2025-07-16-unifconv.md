---
title: Bounding Excess Risk Using Uniform Convergence
date: 2025-07-16 20:00:00 +0800
description: Part 3 of notes on C229M. Bounding the excess risk using uniform convergence techniques.
categories: [ML Theory (C229M T. Ma)]
tags: [mldl]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
$$

Before, we looked at asymptotics as $N \to \infty$. Here, we use the concentration inequalities from [before](https://cogniti0n.github.io/posts/conceneq/) to bound the difference between the empirical loss and the population loss, which is the excess risk. The key idea is to decompose the excess risk.

$$
L(\hat{\theta})-L(\theta^* ) = \left[ L(\hat{\theta})-\hat{L}(\hat{\theta}) \right] + \left[ \hat{L}(\hat{\theta}) - \hat{L}(\theta^* ) \right] + \left[ \hat{L}(\theta^* ) - L(\theta^* ) \right]
$$

The second term is automatically below zero, since $\hat{\theta}$ is the minimizer of $\hat{L}$.

## Bound Using Hoeffding's Inequality

Let’s first look at the third term, which is easier to bound. Apply Hoeffding’s inequality. If we insert $\varepsilon = O\left(\sigma \sqrt{\log{n}}\right)$ where $\sigma^2 = \frac{1}{n}\sum_i (b_i - a_i)^2$, then with high probability ( $>1-2n^{-2c}$ for some large constant $c$ ),

$$
\left|\frac{1}{n}\sum_i X_i - \mu\right| \le O\left(\sigma \sqrt{\log{n}}\right)
$$

Now we assume that the loss takes a value in $[0,1]$, e.g., classification tasks. This assumption will come in handy afterwards. Then  $\sigma = 1/\sqrt{n}$. Hoeffding’s inequality tells us that, with high probability,

$$
\left|\hat{L}(\theta) - L(\theta)\right|\le O\left(\frac{\sqrt{\log{n}}}{\sqrt{n}}\right) = \tilde{O}\left(\frac{1}{\sqrt{n}}\right)
$$

Technically speaking, we have

$$
\mathrm{Pr}\left[\left|\hat{L}(\theta)-L(\theta)\right|\le \tilde{O}\left(1/\sqrt{n}\right)\right] \ge 1-\frac{1}{n^{O(1)}}
$$

## Bound On Arbitrary $\theta$ For Finite Hypothesis Classes

However, we can’t apply the same inequality to $\hat{\theta}$, since the value depends on the data. Mathematicall speaking, this is attributed to the fact that

$$
\forall \theta,\,\mathrm{Pr}\left[\left|\hat{L}(\theta)-L(\theta)\right|\le\varepsilon\right]\ge1-\delta
$$

is not the same as

$$
\mathrm{Pr}\left[\forall \theta,\,\left|\hat{L}(\theta)-L(\theta)\right|\le\varepsilon\right]\ge1-\delta
$$

Therefore, we bound the term using the supremum of the second term with respect to **all** possible $\theta$, i.e.,

$$
L(\hat{\theta})-L(\theta^*) \le \left|\hat{L}(\theta^*) - L(\theta^*)\right| + \sup_{\theta \in \Theta}\left[L(\theta)-\hat{L}(\theta)\right]
$$

>**Theorem** (Bound for finite hypothesis class) Suppose the loss is between 0 and 1 and $h \in \mathcal{H}$ (where $\mathcal{H}$ is finite and $|\mathcal{H}|$ is the cardinality), then 
>$$\forall \delta \in (0,1/2)$$, with probability at least $1-\delta$, we have the following bound.
>
>$$
\left|\hat{L}(h)-L(h)\right|\le\sqrt{\frac{\log|\mathcal{H}|+\log(2/\delta)}{2n}}
>$$

*proof*. By Hoeffding’s inequality, $\forall h \in \mathcal{H}$,

$$
\mathrm{Pr}\left[\left|\hat{L}(h)-L(h)\right|\le\varepsilon\right] \ge 1 - 2\exp(-2n\varepsilon^2)
$$

Now we consider the union bound, i.e.,

$$
\mathrm{Pr}\left[\bigcup^k_{i=1}E_i\right] \le \sum^k_{i=1} \mathrm{Pr}\left[E_i\right]
$$

Apply it to get the uniform convergence bound.

$$
\mathrm{Pr}\left[\exists h \in \mathcal{H} \text{ such that } \left|\hat{L}(h)-L(h)\right|>\varepsilon\right] \le \sum_{h \in \mathcal{H}} 2 \exp(-2n^2\varepsilon^2)=2|\mathcal{H}|\exp(-2n^2\varepsilon^2)
$$

Choose $\varepsilon$ such that 
$$\delta = 2|\mathcal{H}|e^{-2n\varepsilon^2}$$, then

$$
1-\mathrm{Pr}\left[\forall h \in \mathcal{H},\,\left|\hat{L}(h)-L(h)\right|\le\varepsilon\right] \le \delta
$$

The proof is complete. Our bound is logarithmic with $\delta$, so we can safely set $1-\delta \simeq 1$. Therefore, we achieve the bound (with high probability) of

$$
L(\hat{\theta})-L(\theta^*) \le\sqrt{\frac{\log|\mathcal{H}|}{2n}} + O\left(\frac{\sqrt{\log{n}}}{\sqrt{n}}\right)
$$

This is worse than the asymptotic bound, but we have found a dependence on 
$$|\mathcal{H}|$$, i.e., the hypothesis complexity. 

## Bound On Arbitrary $\theta$ For Infinite Hypothesis Classes (Brute-Force Discretization)

Now we turn to infinite hypothesis classes, i.e., continuous classes. First we take a brute-force discretization approach. Suppose $$\mathcal{H}$$
is parameterized by
$$\theta \in \mathbb{R}^p$$.
Consider the bounded parameter space 
$$\Theta = \{ \theta : ||\theta||_2 \le B\}$$.

>**Theorem** Suppose $l((x,y),\theta)$ is $\kappa$-Lipschitz in $\theta$. Then with probability $\ge 1 - O(e^{-p})$,
>
>$$
\forall\theta,\,\left|\hat{L}(\theta)-L(\theta)\right|\le O \left(\sqrt{\frac{p}{n}\,\mathrm{max}\,\{1,\log(\kappa B n)\}}\right)
>$$

_Sketch of the proof_.  The union bound is very pessimistic in the sense that each of the $E_i$s overlap with each other a lot, for close $\theta$. Therefore, we can cover the whole parameter space with much fewer values of $\theta$.

>**Definition** ($\varepsilon$-cover) The $\varepsilon$-cover of a set $S$ with respect to a metric $\rho$ is a set $C$ that satisfies
>
>$$
\forall x \in S, \,\exists x' \in C \text{ such that } \rho(x,x') \le \varepsilon
>$$

i.e., any point in $S$ lies in the closed $\varepsilon$-neighborhood of some point in $C$.

>**Lemma** $\forall \varepsilon \in (0,B)$, $\exists$ an $\varepsilon$-cover of $\Theta$ has at most $(3B/\varepsilon)^p$ elements with 
$$
\rho = ||\cdot||_2
$$.

For now, we won't prove this result, but it can be achieved by discretizing the parameter space and covering the whole space using balls of radius $\varepsilon$. Now we prove the theorem above.

_proof_. First, apply the finite hypothesis analysis to $C$.

$$
\mathrm{Pr}\left[\exists \theta \in \Theta, \, \left|\hat{L}(\theta)-L(\theta)\right|\ge \tilde{\varepsilon} \right] \le 2|C|\exp\left(-2n\tilde{\varepsilon}^2\right)
$$

Next, extend to whole set using the definition of $\varepsilon$-covers. We see that $L(\theta)$ and $\hat{L}(\theta)$ are both $\kappa$-Lipschitz, as they are the sum of $l$s. Take $C$ to be the $\varepsilon$-cover of $S$. Then by definition, $\forall \theta \in \Theta$, $\exists \theta_0 \in C$ such that 
$$
||\theta - \theta_0|| \le \varepsilon
$$.

$$
\hat{L}(\theta) - L(\theta) = \hat{L}(\theta) - \hat{L}(\theta_0) + \hat{L}(\theta_0) -L(\theta_0) + L(\theta_0) - L(\theta)
$$

We apply Lipschitzness to the first and third term. Therefore we can bound the excess risk as

$$
\hat{L}(\theta) - L(\theta) \le 2\kappa \varepsilon + \tilde{\varepsilon} = 2\tilde{\varepsilon}
$$

where we’ve set $\tilde{\varepsilon} = 2\kappa\varepsilon$. The above inequality holds for probability 
$$
2 |C| \exp(-2\tilde{n}\tilde{\varepsilon}^2)
$$
, which we want to bound exponentially, i.e., bound 
$$
2 |C| \exp(-2\tilde{n}\tilde{\varepsilon}^2)<O(\exp(-p))
$$
. Rearranging the inequality, we end up with the following bound.

$$
\log|C| \le p \log{\frac{3B}{\varepsilon}} = p \log{\frac{6B\kappa}{\tilde{\varepsilon}}}
$$

We can finally set

$$
\tilde{\varepsilon} = \sqrt{\frac{c_0 p}{n} \mathrm{max}\left\{1,\log(\kappa B n)\right\}}
$$

where $c_0$ is a sufficiently large constant. We conclude our proof.

## Conclusion

Reviewing the process we have taken above, we have bounded the excess risk by first separating the risk into two parts.

$$
L(\hat{\theta})-L(\theta^*) \le \left| L(\theta^*) - \hat{L}(\theta^*) \right| + \sup_{\theta \in \Theta}(L(\theta) - \hat{L}(\theta))
$$

Each of the terms are bounded by

$$
\left| \hat{L}(\theta^*) - L(\theta^*) \right| \le O\left(\frac{\sqrt{\log{n}}}{\sqrt{n}}\right) = \tilde{O}\left(\frac{1}{\sqrt{n}}\right)
$$

$$
\sup_{\theta \in \Theta}\left[ \hat{L}(\theta)-L(\theta) \right] \le O \left(\sqrt{\frac{p}{n}\,\mathrm{max}\,\{1,\log(\kappa B n)\}}\right)
$$

Since our bound is of order $1/\sqrt{n}$, it is still worse than the asymptotic bound from [before](https://cogniti0n.github.io/posts/sup_asymp/). However, we also see that the dimension of $\theta$ is present. This is an important factor - the dimension of the parameter space (or the complexity of the model) is directly related to generalization.

## References
[1] Tengyu Ma, Lecture notes for "CS229M (Machine Learning Theory)"