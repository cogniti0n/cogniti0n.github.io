---
title: Resolvents, Proximal Point Method, and Operator Splitting
date: 2025-08-12 05:50 +0800
description: Introduction to the notion of resolvents, and methods such as PPM and operator splitting.
categories: [optimization, convex-optimization]
tags: [lecture-notes]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

## Resolvents

We first define the _resolvent_ of an operator $A$.

> **Definition** The _resolvent_ of $A$ is defined as the operator
>
> $$
J_A = (I + A)^{-1}
> $$
>
> The _reflected resolvent_ of $A$ is defined as the operator
>
> $$
R_A = 2J_A - I
> $$

We often use $J_{\alpha A}$ and $R_{\alpha A}$ with $\alpha > 0$. 

> **Theorem** If $A$ is maximal monotone, then $R_A$ is nonexpansive with domain $\R^n$, and $J_A$ is $1/2$-averaged with domain $\R^n$.

_proof_. Let $(x,u),(y,v) \in J_A$. Then by definition $x \in u + Au$ and $y \in v + Av$. By monotonicity,

$$
\langle (x-u) - (y-v), u-v \rangle \ge 0
$$

and

$$
\| (2u-x) - (2v-y) \|^2 = \| x-y \|^2 - 4 \langle (x-u) - (y-v) , u-v \rangle \le \| x-y \|^2
$$

Therefore, $R_A$ is nonexpansive and $J_A = \frac{1}{2} I + \frac{1}{2} R_A$.

The domain of $R_A$ and $J_A$ being $\R^n$ requires maximal monotone, and is a consequence of the Minty surjectivity theorem. This theorem is difficult to prove at this stage.

> **Theorem** The zero set of $A$ is closed and convex when $A$ is maximal monotone.

_proof_. $\mathrm{Zer}\,A = \mathrm{Fix}\,J_A$ since

$$
0 \in Ax \Leftrightarrow x \in x + Ax \Leftrightarrow J_Ax = x
$$

Since $J_A$ is nonexpansive, $\mathrm{Fix}\,J_A$ is closed convex. This proof relies on maximality, from the domain condition. 

If $A$ is a symmetric matrix, then the eigenvalues of $A$ is in $[0,\infty)$. $J_A$ has eigenvalues in $(0,1]$, and $R_A$ has eigenvalues in $(-1,1]$.

> **Theorem** For CCP $f$ and $\alpha > 0$,
>
> $$
J_{\alpha \partial f} = \mathrm{Prox}_{\alpha f}
> $$

_proof_. 

$$
\begin{aligned}
z = (I + \alpha \partial f)^{-1}(x) &\Leftrightarrow z + \alpha \partial f(z) \in x \\
                                    &\Leftrightarrow 0 \in \partial_z \left( \alpha f(x) + \frac{1}{2} \| z-x \|^2 \right) \\
                                    &\Leftrightarrow z = \argmin_{z} \left\{ \alpha f(z) + \frac{1}{2} \| z-x \|^2 \right\} \\
                                    &\Leftrightarrow z = \mathrm{Prox}_{\alpha f}(x)
\end{aligned}
$$

> **Proposition** If $g(u) = f^* (A^\top u)$, $f$ is CCP, and regularity conditions ($\mathrm{ri}\,\dom\,f^* \cap \mathrm{R}(A^\top) \ne \emptyset$) hold, then
>
> $$
v = \mathrm{Prox}_{\alpha g}(u) \Leftrightarrow \text{for some } x \in \argmin_x \left\{ f(x) - \langle u,Ax \rangle + \frac{\alpha}{2} \| Ax \|^2 \right\} \text{ , } v = u - \alpha Ax
> $$

_proof_. Use the theorem above, then $v = J_{\alpha \partial g}(u)$. Therefore, the following holds.

$$
\begin{aligned}
v = (I+\alpha\partial g)^{-1}(u) &\Leftrightarrow u \in v + \alpha g(v) = v + \alpha A \partial f^* (A^\top v) \\
                                 &\Leftrightarrow \exists \, x \in \partial f^*(A^\top u) \text{ s.t. } u = v + \alpha Ax \\
\end{aligned}
$$

Now from the definition of the dual function,

$$
A^\top v = A^\top (u - \alpha Ax) \in \partial f(x)
$$

which is equivalent to

$$
x \in \argmin_x \left\{ f(x) - \langle u,Ax \rangle + \frac{\alpha}{2} \| Ax \|^2 \right\}
$$

If $C \in \R^n$ is nonempty and closed convex, then $J_{\partial \delta_C} = \mathrm{Prox}_{\delta_C}$ is the projection onto $C$, i.e., $\Pi_C$. Therefore the resolvent generalizes projection. 

Here is an example that we will use later on. I omitted the proof.

> **Theorem** Consider the Lagrangian for an optimization problem with linear constraints,
>
> $$
L(x,u) = f(x) + \langle u,Ax-b \rangle
> $$
>
> Then, the resolvent of $\partial L$, which is defined as
>
> $$
\partial L = \begin{bmatrix} \partial_x L \\ \partial_u (-L) \end{bmatrix}
> $$
>
> can be computed using the following identity.
>
> $$
J_{\alpha \partial L}(x,u) = (y,v) \Leftrightarrow \text{for some } y \in \argmin_{z} \left\{ L_\alpha(z,u) + \frac{1}{2\alpha} \| z-x \|^2 \right\} \text{ , } v = u + \alpha (Ay-b)
> $$
>
> where
>
> $$
L_\alpha(x,u) = L(x,u) + \frac{1}{2}\alpha \|Ax-b \|^2
> $$

Below are some identities that hold for resolvents. Let $A$ be maximal monotone and $\alpha > 0$, then
- If $B(x) = A(x) + t$, then $J_{\alpha B}(u) = J_{\alpha A}(u-\alpha t)$
- If $B(x) = A(x-t)$, then $J_{\alpha B}(u) = J_{\alpha A}(u-t)+t$
- If $B(x) = -A(t-x)$, then $J_{\alpha B}(u) = t-J_{\alpha A}(t-u)$
- (_Inverse resolvent identity_) 

$$
J_{\alpha^{-1}A}(x) + \alpha^{-1}J_{\alpha A^{-1}}(\alpha x) = x
$$

- If $\alpha = 1$ in the identity above, then $J_{A} + J_{A^{-1}} = I$.
- If $f$ is CCP in the identity above, then 
$$\mathrm{Prox}_{\alpha^{-1}f}(x) + \alpha^{-1}\mathrm{Prox}_{\alpha f^* }(\alpha x) = x$$
- (_Reflected resolvent identity 1_)

$$
R_{\alpha A} = (I - \alpha A)(I + \alpha A)^{-1}
$$

The proof uses a lemma which will be useful later on.

> **Lemma** If $x \in \dom{T}$ and $T^{-1}$ is single-valued, then $T^{-1}Tx = x$.

_proof_. There exists some $y$ such that $y \in Tx$. Therefore, $x \in T^{-1}y \in T^{-1}(Tx)$. Now we prove that such $x$ is unique. Suppose $\tilde{x} \in T^{-1}Tx$. Then, there exists some $y$ such that $\tilde{x} = T^{-1}y$, and therefore $y \in Tx$, and consequently $T^{-1}y=x$. Therefore, $\tilde{x} = x$ and $x$ is the unique element of $T^{-1}Tx$.

Now we prove that identity stated above.

_proof_. 

$$
\begin{aligned}
R_{\alpha A} &= 2(I + \alpha A)^{-1} - I \\
             &= 2(I + \alpha A)^{-1} - (I + \alpha A)(I + \alpha A)^{-1} \\
             &= (I - \alpha A)(I + \alpha A)^{-1}
\end{aligned}
$$

The second line holds from the lemma above and the maximal monotone property of $A$.

- (_Reflected resolvent identity 2_)

$$
R_{\alpha A}(I + \alpha A) = I - \alpha A
$$

_proof_. For some $x \in \dom\,A$ (if $x \notin \dom\,A$, then both sides are empty sets), 

$$
\begin{aligned}
R_{\alpha A}(I + \alpha A)(x) &= 2(I + \alpha A)^{-1}(I + \alpha A)(x) - (I + \alpha A)(x) \\
                              &= 2I(x) - (I + \alpha A)(x) \\
                              &= (I - \alpha A)(x)
\end{aligned}
$$

Again, the second line holds from the lemma above and maximal monotone property of $A$.

## Proximal Point Method

Consider the problem of finding zeros of a maximally monotone operator $A$. Then such a problem is equivalent to finding the fixed point of $J_{\alpha A}$, for some $\alpha > 0$. 

> **Definition** The fixed point iteration
> 
> $$
x^{k+1} = J_{\alpha A}(x^k)
> $$
>
> is the proximal point method (PPM) or proximal minimization.

Since $J_{\alpha A}$ is averaged, PPM converges given the existance of a solution. Below are some examples of problems that can be solved using PPM.

#### Method of Multipliers (MM)

Consider the primal-dual pair generated by

$$
L(x,u) = f(x) + \langle u, Ax-b \rangle
$$

Then recall the example above - the augmented Lagrangian

$$
L_\alpha(x,u) = L(x,u) + \frac{1}{2}\alpha \|Ax-b \|^2
$$

Then our goal is, given a primal-dual solution pair exist, to find the zero of $\partial L$. Consequently, we want to find the fixed points of $J_{\alpha \partial A}$. Therefore, we can use the results from the example above. First assume the regularity condition holds. Then write $g(u) = f^* (=A^\top u) + b^\top u$. We end up with the following fixed-point iteration:

$$
x^{k+1} \in \argmin_{x}\,L_\alpha(x,u^k)
$$

$$
u^{k+1} = u^k + \alpha (Ax^{k+1} - b)
$$

If a dual solution exists and $\alpha > 0$, then $u^k \to u^\star $.

#### Proximal Method of Multipliers

We consider a similar fixed point iteration:

$$
x^{k+1} = \argmin_x \left\{ L_\alpha(x,u^k) + \frac{1}{2\alpha} \| x-x^k \|^2 \right\}
$$

$$
u^{k+1} = u^k + \alpha (Ax^{k+1} - b)
$$

If total duality holds and $\alpha > 0$, then $x^k \to x^\star$ and $u^k \to u^\star$.

## Operator Splitting

TODO

## References
[1] Ryu, Ernest K., and Wotao Yin. 2022. Large-Scale Convex Optimization. Cambridge, England: Cambridge University Press.