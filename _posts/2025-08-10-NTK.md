---
title: Neural Tangent Kernels
date: 2025-08-10 05:50 +0800
description: Neural tangent kernel formulation.
categories: [optimization, ml-optimization]
tags: [lecture-notes]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\dom{\mathrm{dom}}
    \def\R{\mathbb{R}}
$$

Neural tangent kernels (NTKs) are a powerful tool that can approximate a nonconvex (even locally) function with a convex optimization problem. Given an initialization $\theta_0$, we pick a neighborhood around $\theta_0$ that is convex and has a global minimum. To do this, we linearize the loss function, i.e.,

$$
g_\theta(x) \equiv f_{\theta_0}(x) + \langle \nabla_\theta f_{\theta_0}(x), \theta-\theta^0 \rangle
$$

Then $g$ becomes an affine function in $\theta$, and $\phi(x) \equiv \nabla_{\theta_0}(x)$ becomes a function of $x$. 

> **Definition** The _neural tangent kernel_ is defined as the following kernel.
>
> $$
K(x,x') = \langle \phi(x), \phi(x') \rangle = \langle \nabla_\theta f_{\theta_0}(x), \nabla_\theta f_{\theta_0}(x') \rangle
> $$

Instead of minimizing the loss $\ell(f_\theta(x),y)$, we minimize $\ell(g_\theta(x),y)$ - the validity of such a minimization will be stated later.

## NTK For Quadratic Loss

For a given dataset $$ \mathcal{D} = \{ x_i,y_i \}^n_{i=1} $$, we define $\phi_i = \phi(x_i) = \nabla_{\theta} f_{\theta_0}(x_i) \in \R^p$ and denote

$$
\Phi \equiv \begin{bmatrix} \phi_i^\top \\ \vdots \\ \phi_n^\top \end{bmatrix} \in \R^{n\times p}
$$

If we take the quadratic loss,

$$
\hat{L} = \frac{1}{n} \sum^n_{i=1} \left( y_i - \phi(x_i)^\top \Delta \theta \right)^2 = \frac{1}{n} \| y - \Phi \Delta \theta \|^2_2
$$

where $y = [y_1, \dots, y_n]^\top \in \R^n$ and $\Delta \theta = \theta - \theta_0$. This is a linear regression problem in $\Delta \theta$. If we assume $y_i = O(1)$ and 
$$\|y\|_2 = O(\sqrt{n})$$
, then the following bound can be proved.
> **Theorem** If $p \ge n$ (overparametrization), $\mathrm{rank}\,\Phi = n$, and $\sigma_{\mathrm{min}}(\Phi) = \sigma > 0$, then the minimum norm solution $\Delta \hat{\theta}$ satisfies the following inequality.
>
> $$
\| \Delta\hat{\theta} \|_2 \le O(\sqrt{n}/\sigma)
> $$

_proof_. Let $\Phi^+$ be the pseudoinverse of $\Phi$. Then $\Delta \hat{\theta} = \Phi^+ y$ and $$\| \Phi^+ \|_{\mathrm{op}} = 1/\sigma$$. Therefore,

$$
\| \Delta \hat{\theta} \|_2 \le \| \Phi^+ \|_\mathrm{op} \cdot \| y \|_2 \le O\left( \frac{1}{\sigma} \cdot \sqrt{n} \right)
$$

Thus, the nearest global solution exists in a ball around $\theta_0$ whose radius is bound.

## Validity of NTKs

What is the bound around $\theta_0$, where the NTK approximation is similar to the original function $f_\theta(x)$?

> **Lemma** Suppose $\nabla_\theta f_\theta(x)$ is $\beta$-Lipschitz in $\theta$, then 
>
> $$
\| f_\theta(x) - g_\theta(x) \| \le O(\beta \|\Delta \theta\|^2_2)
> $$
>
> If we restrict $\theta$ to 
$$\{\theta_0 + \Delta \theta \, | \, \| \Delta \theta \|_2 \le O(\sqrt{n}/\sigma) \}$$
> then we can bound
>
> $$
\| f_\theta(x) - g_\theta(x) \| \le O\left( \frac{\beta n}{\sigma^2} \right)
> $$

_proof_. 

$$
\left| f_\theta(x) - g_\theta(x) \right| = \left| h(\theta) - h(\theta_0) - \left\langle \nabla h(\theta_0), \theta-\theta_0 \right\rangle \right| \le O\left( \beta \|\theta-\theta_0\|^2_2 \right)
$$

which is from $\beta$-Lipschitzness of $\nabla h(\theta)$.

## NTK Regime Analysis

#### Reparametrization

(See [2]) Let $f_\theta(x) = \alpha \bar{f}_\theta(x)$ where $\bar{f}$ is fixed, and only vary the scaling $\alpha$. If we fix $\theta_0$, we easily see that $\sigma = \alpha \bar{\sigma}$ and $\beta = \alpha \bar{\beta}$. Therefore,

$$
\frac{\beta}{\sigma^2} = \frac{\bar{\beta}}{\bar{\sigma}^2}\cdot\frac{1}{\alpha} \to 0 \quad \text{as} \quad \alpha \to \infty
$$

Therefore the NTK region becomes bigger for larger $\alpha$.

#### Wide Neural Nets

Consider a two-layer neural net with $m$ neurons. Then, the output becomes

$$
\hat{y} = \frac{1}{\sqrt{m}} \sum^m_{i=1} a_i \sigma(w_i^\top x)
$$

where $w_i \in R^d$, $\sigma$ is 1-Lipschitz and twice-differentiable, $$a_i \sim \{ \pm 1 \}$$, $w_i^0 \sim \mathcal{N}(0,I)$, $$ \| x \|_2 = \Theta(1)$, and $\theta$ is the vectorized version of $W = [w_1^\top, \dots, w_m^\top]^\top$, i.e., $\theta \in \R^{dm}$.

## References
[1] Ma, Tengyu. 2022. Lecture notes for CS229M (Machine Learning Theory) \
Chizat, Lenaic, Edouard Oyallon, and Francis Bach. 2018. “On Lazy Training in Differentiable Programming.” arXiv [Math.OC]. arXiv. http://arxiv.org/abs/1812.07956.
