---
title: Nesterov's Accelerated Gradient Method
date: 2025-08-07 05:50 +0800
description: A brief summary of Nesterov's accelerated gradient method.
categories: [optimization, convex-optimization]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\R{\mathbb{R}}
$$

## Nesterov's Accelerated Gradient Method

Nesterov's accelerated gradient method is defined as follows.

> **Definition** Assume $f$ is $L$-smooth and convex. Start from an initial value $x_0$ and $y_0=x_0$. Then iteratively define
>
> $$
y_k = x_k - \frac{1}{L} \nabla f(x_k)
> $$
>
> $$
x_{k+1} = y_k + \frac{\theta_k-1}{\theta_{k+1}} (y_k - y_{k-1})
> $$
>
> Where $\theta_0 = 1$ and $\theta_i = \frac{1 + \sqrt{1 + 4\theta_i^2}}{2}$ for $i = 1,\dots,N$.

Then Nesterov's acelerated gradient method gives us a faster convergence rate compared to regular gradient-descent based methods.

> **Theorem** Nesterov's accelerated gradient method gives us the convergence rate
>
> $$
f(y_k) - f^\star \le O\left( \frac{1}{k^2} \right)
> $$
>
> Which is the optimal rate for first-order methods.

_proof_. Convexity give us

$$
-f(y) \le -f(x) + \langle \nabla f(x) , x-y \rangle
$$

and smoothness gives us

$$
f\left( x - \frac{1}{L} f(x) \right) - f(x) \le -\frac{1}{L} \| \nabla f(x) \|_2^2 + \frac{1}{2L} \| \nabla f(x) \|^2_2 = -\frac{1}{2L} \| \nabla f(x) \|^2_2
$$

Then put $x \gets x_k$, $y \gets x^\star$, and $x \gets x_k$, $y \gets y_k$, which leads to

$$
f\left(x_k - \frac{1}{L} \nabla f(x) \right) - f^\star \le -\frac{1}{2L} \| \nabla f(x_k) \|_2^2 + \langle \nabla f(x_k) , x_k-x^\star \rangle
$$

$$
f\left(x_k - \frac{1}{L}\nabla f(x_k)\right) - f(y_k) \le -\frac{1}{2L}\| \nabla f(x_k) \|^2_2 + \langle \nabla f(x_k), x_k-y_k \rangle
$$

Simplify: $\delta_k \gets f(y_k) - f^\star$. Then,

$$
\delta_{k+1} = f\left(x_k - \frac{1}{L} \nabla f(x) \right)
$$

$$
\delta_{k+1} - \delta{k} = f\left(x_k - \frac{1}{L}\nabla f(x_k)\right) - f(y_k)
$$

Also from definition,

$$
\nabla f(x_k) = -L(y_{k+1}-y_k)
$$

$$
\| \nabla f(x_k) \|^2_2 = L^2 \| y_{k+1}-x_k \|^2_2
$$

Therefore we end up with

$$
\delta_{k+1} \le -\frac{L}{2} \| y_{k+1}-y_k \|^2_2 - L \langle y_{k+1}-x_k, x_k-x^\star \rangle
$$

$$
\delta_{k+1}-\delta_k \le -\frac{L}{2} \| y_{k+1}-x_k \|^2_2 - L \langle y_{k+1}-x_k, x_k-y_k \rangle
$$

Now comes the tricky step: consider the former of the two inequalities above $+ (\theta_k-1) \times$ the latter. Then the left hand side of the inequality simplifies to $\theta_k \delta_{k+1} - (\theta_k - 1)\delta_k$. Once the right hand side is also simplified, the inequality becomes

$$
\theta_k \delta_{k+1} - (\theta_k - 1) \delta_k \le - \frac{\theta_k L}{2} \| y_{k+1}-x_k \|^2_2 - L \langle y_{k+1} - x_k, \theta_k x_k - (\theta_k - 1) y_k - x^\star \rangle
$$

From definition of the coefficients, we have $\theta_{k-1}^2 = \theta_k (\theta_k-1)$, multiply both sides of the inequality above with $\theta_k$ and put the $\theta_{k-1}^2 = \theta_k (\theta_k-1)$ in:

$$
\theta_k^2 \delta_{k+1} - \theta^2_{k-1} \delta_k \le -\frac{L}{2} \left( \theta^2_k \| y_{k+1}-x_k \|^2_2 + 2\theta_k \langle y_{k+1}-x_k, \theta_k x_k - (\theta_k-1)y_k - x^\star \rangle \right)
$$

Once examining the right hand side, the equation becomes (_not obvious in my eyes..._)

$$
\theta_k^2 \delta_{k+1} - \theta^2_{k-1} \delta_k \le -\frac{L}{2} \left( \|\theta_k y_{k+1} - (\theta_k - 1)y_k - x^\star \|^2_2 - \| \theta_k x_k - (\theta_k - 1)y_k - x^\star \|^2_2 \right)
$$

We also have

$$
\theta_k x_k - (\theta_k - 1)y_k = (1-\theta_{k-1})y_{k-1} + \theta_{k-1}y_k
$$

which stems from $\theta_{k+1}x_{k+1} - \theta_{k+1}y_{k+1} = (1-\theta_k)(y_k - y_{k+1})$. Therefore the result from above becomes

$$
\theta^2_k \delta_{k+1} - \theta^2_{k-1}\delta_k \le -\frac{L}{2} \left( \|\theta_k y_{k+1} - (\theta_k - 1)y_k - x^\star \|^2_2 - \| (1-\theta_{k-1})y_{k-1} + \theta_{k-1}y_k - x^\star \|^2_2 \right)
$$

Perform telescoping sum on both sides. Use $\theta_0 = 0$, $y_0 = x_0$, $\theta_0 y_1 - (\theta_0 -1)y_0 - x^\star = x_0 - x^\star$. Therefore, if we denote the final index as $K$,

$$
\theta_{K-1}^2 \delta_K \le \frac{L}{2} \| x_0 - x^\star \|^2_2
$$

A simple proof by induction leads to $\theta_{k-1} \ge k/2$. Therefore,

$$
f(y_K) - f^\star \le \frac{2L \|x_0 - x^\star \|^2_2}{K^2}
$$

To better understand the accelerated gradient method, define the weight $\gamma_k = (1-\theta_k) / \theta_{k+1}$, the the method becomes an extrapolation

$$
x_{k+1} = (1-\gamma_k)y_{k+1} + \gamma_k y_k
$$

## Alternative Accelerated Gradient Method

Below is an alternative definition, that also enjoys the same convergence rate (moreover, they are identital in an asymptotic sense). This method is defined more straightforward, and also is the basis for the continuous-time analogy stated below.

> **Definition** (Alternative AGM) Consider the minimization of a $L$-smooth and convex function $f(x)$. Then define the iterative method
>
> $$
x^{k+1} = y^k - \frac{1}{L} \nabla f(y^k)
> $$
>
> $$
y^{k+1} = x^{k+1} + \frac{k-1}{k+2} (x^{k+1} - x^k)
> $$
>
> where $x^0 = y^0 \in \R^d$.

Then as stated before, this method has convergence rate

$$
f(x^k) - f(x^\star) \le \frac{2L\|x^0 - x^\star\|^2}{k^2}
$$

## Lower-Bound Proof

We now prove that Nesterov's accelerated gradient method is the _best_ first-order algorithm - more specifically, the convergence rate $O(1/k^2)$ is optimal.

We first construct a black-box for a first-order algorithm. Take the iteration incorporates all possible first-order methods. Now we prove that there exists a worst-case scenario, no matter the algorithm, that exhibits the convergence rate of $1/k^2$.

$$
x_{k+1} \in x_0 + \mathrm{span}\left\{ g_1,\dots,g_k \right\}
$$

where $g_i \in \partial f(x_i)$.

> **Theorem** There exists a function $f$ that is $L$-smooth and convex such that any first-order algorithm satisfies
>
> $$
f(x_k) - f(x^\star) \ge \frac{3LR_0^2}{32(1+k^2)}
> $$
>
> for $R_0 = \|x_0 - x^\star\|_2$.

_proof_. The proof is constructed by explicitly constructing the worst-case function, called _Nesterov's worst function_. Before we begin the proof, we assume that $2k+1 \le n$, i.e., the iteration number is less than half of the dimension. To me, this assumption felt _ad hoc_, and moreover this assumption is absolutely necessary. In Nesterov's words,

> Firstly,  they describe  the potential performance of numerical  methods on  the  initial stage of the minimization process.  And secondly,  they warn  us  that without a  direct use of finite-dimensional arguments we  cannot get  better complexity for  any numerical scheme. [1]

If $2k+1 < n$, then we can easily see that the 'leftover dimensions' can be ignored (see [1] for the full proof). Therefore, we set $2k+1 = n$ for simplicity. First define the tridiagonal matrix $A_n \in \R^{n \times n}$.

$$
A_n =
\begin{bmatrix}
2 & -1 &  & \cdots & \\
-1 & 2 & -1 & \cdots & \\
 & -1 & 2 & \cdots &  \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
 & & & \cdots & 2
\end{bmatrix}
$$

Now define the function

$$
f(x) = \frac{L}{8}x^\top A x - \frac{L}{4}x^\top e_1
$$

The minimizer of $f$ can be computed easily. The minimizer $x^\star$ becomes

$$
(x^\star)^{(i)} = 1 - \frac{i}{n+1}
$$

and the optimal value

$$
f(x^\star) = - \frac{L}{8}\left( 1 - \frac{1}{n+1} \right)
$$

Assuming we start with $x_0 = 0$ for simplicity, we have (using the general definition of a first-order method)

$$
x_t \in \mathrm{span}\left\{ e_1,\dots,e_{t-1} \right\}
$$

Now we have $x_k^{(i)} = 0$ for any $i \ge k$. If we consider the subspace, for which the values of all $i \ge k$ indices are set to zero, denoted as $\R^{k,n}$, define

$$
x^\star_k = \argmin_{x \in \R^{k,n}}\,f(x)
$$

Now we find that $(x_k^\star)^{(i)} = 1 - \frac{i}{k+1}$ if $i < k$ and 0 otherwise. This gives the cost

$$
f(x_k^\star) = -\frac{L}{8}\left(1 - \frac{1}{k+1}\right)
$$

Therefore,

$$
f(x_k) - f(x^\star) \ge f(x_k^\star) - f(x^\star) = \frac{L}{8}\left( \frac{1}{k+1} - \frac{1}{n+1} \right) = \frac{L}{16(k+1)}
$$

where we used $n = 2k+1$. Also we can bound (using the assumption $x_0 = 0$)

$$
R_0^2 = \| x_0 - x^\star \|^2_2 = \| x^\star \|_2^2 = \sum^n_{i=1} \left( 1 - \frac{i}{n+1} \right)^2 \le \frac{2(k+1)}{3}
$$

(second inequality can be derived using some basic algebra). Therefore we have the final lower bound

$$
f(x_k) - f(x^\star) \ge \frac{3LR_0^2}{32(k+1)^2}
$$

Keep in mind that, when the assumption of large dimension breaks down, so does the lower bound of $O(1/k^2)$.

## Continuous-Time Analogous of Acceleration

THe method proposed above can be effectively viewed as the discretization of a second-order ODE,

$$
\ddot{X} + \frac{3}{t} \dot{X} + \nabla f(X) = 0
$$

with initial conditions $X(0) = x_0$, $\dot{X}(0) = 0$. Such an ODE also enjoys inverse quadratic convergence rate.

$$
f(X(t)) - f^\star \le O \left( \frac{\| x_0 - x^\star \|^2_2}{t^2} \right)
$$

To give an informal derivation, begin with the alternative AGM mentioned above. If the two equations are combined, then

$$
\frac{x_{k+1}-x_k}{\sqrt{s}} = \frac{k+1}{k+2} \frac{x_k - x_{k-1}}{\sqrt{s}} - \sqrt{s}\nabla f(y_k)
$$

Then introduc the _Ansatz_ $x_k \approx X(k\sqrt{s})$ for some smooth curve $X(t)$, and denote $t = k\sqrt{s}$. Then as $s \to 0$, $X(t) \approx x_k$ and $X(t+\sqrt{s}) \approx x_{k+1}$. Therefore, we can linearize

$$
\frac{x_{k+1} - x_k}{\sqrt{s}} = \dot{X}(t) + \frac{1}{2} \ddot{X}(t)\sqrt{s} + o(\sqrt{s})
$$

$$
\frac{x_k - x_{k-1}}{\sqrt{s}} = \dot{X}(t) - \frac{1}{2} \ddot{X}(t)\sqrt{s} + o(\sqrt{s})
$$

Moreover, $\sqrt{s}\nabla f(y_k) = \sqrt{s} \nabla f(X(t)) + o(\sqrt{s})$. If we use these approximations and compare the coefficients of $\sqrt{s}$, then we get the resulting ODE. Our initial definition of AGM also yields the exact same ODE, as the coefficient $\theta_k (\theta_{k-1}^{-1} - 1)$ asymptotically is equal to $1 - 3/k + O(1/k^2)$.

> **Theorem** Let $X(t)$ be the unique solution to the ODE with initial conditions $X(0) = x_0$, $\dot{X}(0) = 0$. Then for any $t > 0$,
>
> $$
f(X(t)) - f^\star \le \frac{2 \| x_0 - x^\star \|^2}{t^2}
> $$

_proof_. Consider the energy functional

$$
\mathcal{E}(t) = t^2\left( f(X(t)) - f^\star \right) + 2 \left\| X + \frac{1}{2}t\dot{X} - x^\star \right\|^2
$$

whose time derivative is

$$
\dot{\mathcal{E}} = 2t(f(X) - f^\star) + t^2 \langle \nabla f, \dot{X} \rangle + 4 \left\langle X + \frac{t}{2}\dot{X} - x^\star, \frac{3}{2}\dot{X} + \frac{t}{2} \ddot{X} \right\rangle
$$

Using the ODE equation and the convexity of $f$, the above equation yields

$$
\dot{\mathcal{E}} = 2t(f(X) - f^\star) - 2t \langle X-x^\star, \nabla f(X) \rangle \le 0
$$

Hence,

$$
f(X(t)) - f^\star \le \frac{\mathcal{E}(t)}{t^2} \le \frac{\mathcal{E}(0)}{t^2} = \frac{2 \| x_0 - x^\star \|^2}{t^2}
$$

## References

[1] Nesterov, Y. (2013). Introductory lectures on convex optimization: A basic course (2004th ed.) [PDF]. Springer.\
[2] Su, W., Boyd, S., & Candes, E. J. (2015). A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights. In arXiv [stat.ML]. arXiv. http://arxiv.org/abs/1503.01243
