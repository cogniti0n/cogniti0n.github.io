---
title: Nesterov's Accelerated Gradient Method
date: 2025-08-07 05:50 +0800
description: A brief summary of Nesterov's accelerated gradient method.
categories: [optimization, convex-optimization]
math: true
toc: false
---

$$
    \def\argmin{\mathop{\mathrm{argmin}}}
    \def\argmax{\mathop{\mathrm{argmax}}}
    \def\expectation{\mathop{\mathbb{E}}}
    \def\R{\mathbb{R}}
$$

## Nesterov's Accelerated Gradient Method

Nesterov's accelerated gradient method is defined as follows.

> **Definition** Assume $f$ is $L$-smooth and convex. Start from an initial value $x_0$ and $y_0=x_0$. Then iteratively define
>
> $$
y_k = x_k - \frac{1}{L} \nabla f(x_k)
> $$
>
> $$
x_{k+1} = y_k + \frac{\theta_k-1}{\theta_{k+1}} (y_k - y_{k-1})
> $$
>
> Where $\theta_0 = 1$ and $\theta_i = \frac{1 + \sqrt{1 + 4\theta_i^2}}{2}$ for $i = 1,\dots,N$.

> **Theorem** Nesterov's accelerated gradient method gives us the convergence rate
>
> $$
f(y_k) - f^\star \le O\left( \frac{1}{k^2} \right)
> $$
>
> Which is the optimal rate for first-order methods.

_proof_. Convexity give us

$$
-f(y) \le -f(x) + \langle \nabla f(x) , x-y \rangle 
$$

and smoothness gives us

$$
f\left( x - \frac{1}{L} f(x) \right) - f(x) \le -\frac{1}{L} \| \nabla f(x) \|_2^2 + \frac{1}{2L} \| \nabla f(x) \|^2_2 = -\frac{1}{2L} \| \nabla f(x) \|^2_2
$$

Then put $x \gets x_k$, $y \gets x^\star$, and $x \gets x_k$, $y \gets y_k$, which leads to

$$
f\left(x_k - \frac{1}{L} \nabla f(x) \right) - f^\star \le -\frac{1}{2L} \| \nabla f(x_k) \|_2^2 + \langle \nabla f(x_k) , x_k-x^\star \rangle
$$

$$
f\left(x_k - \frac{1}{L}\nabla f(x_k)\right) - f(y_k) \le -\frac{1}{2L}\| \nabla f(x_k) \|^2_2 + \langle \nabla f(x_k), x_k-y_k \rangle
$$

Simplify: $\delta_k \gets f(y_k) - f^\star$. Then,

$$
\delta_{k+1} = f\left(x_k - \frac{1}{L} \nabla f(x) \right)
$$

$$
\delta_{k+1} - \delta{k} = f\left(x_k - \frac{1}{L}\nabla f(x_k)\right) - f(y_k)
$$

Also from definition,

$$
\nabla f(x_k) = -L(y_{k+1}-y_k)
$$

$$
\| \nabla f(x_k) \|^2_2 = L^2 \| y_{k+1}-x_k \|^2_2
$$

Therefore we end up with

$$
\delta_{k+1} \le -\frac{L}{2} \| y_{k+1}-y_k \|^2_2 - L \langle y_{k+1}-x_k, x_k-x^\star \rangle
$$

$$
\delta_{k+1}-\delta_k \le -\frac{L}{2} \| y_{k+1}-x_k \|^2_2 - L \langle y_{k+1}-x_k, x_k-y_k \rangle
$$

Now comes the tricky step: consider the former of the two inequalities above $+ (\theta_k-1) \times$ the latter. Then the left hand side of the inequality simplifies to $\theta_k \delta_{k+1} - (\theta_k - 1)\delta_k$. Once the right hand side is also simplified, the inequality becomes

$$
\theta_k \delta_{k+1} - (\theta_k - 1) \delta_k \le - \frac{\theta_k L}{2} \| y_{k+1}-x_k \|^2_2 - L \langle y_{k+1} - x_k, \theta_k x_k - (\theta_k - 1) y_k - x^\star \rangle
$$

From definition of the coefficients, we have $\theta_{k-1}^2 = \theta_k (\theta_k-1)$, multiply both sides of the inequality above with $\theta_k$ and put the $\theta_{k-1}^2 = \theta_k (\theta_k-1)$ in:

$$
\theta_k^2 \delta_{k+1} - \theta^2_{k-1} \delta_k \le -\frac{L}{2} \left( \theta^2_k \| y_{k+1}-x_k \|^2_2 + 2\theta_k \langle y_{k+1}-x_k, \theta_k x_k - (\theta_k-1)y_k - x^\star \rangle \right)
$$

Once examining the right hand side, the equation becomes (_not obvious in my eyes..._)

$$
\theta_k^2 \delta_{k+1} - \theta^2_{k-1} \delta_k \le -\frac{L}{2} \left( \|\theta_k y_{k+1} - (\theta_k - 1)y_k - x^\star \|^2_2 - \| \theta_k x_k - (\theta_k - 1)y_k - x^\star \|^2_2 \right)
$$

We also have 

$$
\theta_k x_k - (\theta_k - 1)y_k = (1-\theta_{k-1})y_{k-1} + \theta_{k-1}y_k
$$

which stems from $\theta_{k+1}x_{k+1} - \theta_{k+1}y_{k+1} = (1-\theta_k)(y_k - y_{k+1})$. Therefore the result from above becomes

$$
\theta^2_k \delta_{k+1} - \theta^2_{k-1}\delta_k \le -\frac{L}{2} \left( \|\theta_k y_{k+1} - (\theta_k - 1)y_k - x^\star \|^2_2 - \| (1-\theta_{k-1})y_{k-1} + \theta_{k-1}y_k - x^\star \|^2_2 \right)
$$

Perform telescoping sum on both sides. Use $\theta_0 = 0$, $y_0 = x_0$, $\theta_0 y_1 - (\theta_0 -1)y_0 - x^\star = x_0 - x^\star$. Therefore, if we denote the final index as $K$,

$$
\theta_{K-1}^2 \delta_K \le \frac{L}{2} \| x_0 - x^\star \|^2_2
$$

A simple proof by induction leads to $\theta_{k-1} \ge k/2$. Therefore,

$$
f(y_K) - f^\star \le \frac{2L \|x_0 - x^\star \|^2_2}{K^2}
$$

To better understand the accelerated gradient method, define the weight $\gamma_k = (1-\theta_k) / \theta_{k+1}$, the the method becomes an extrapolation

$$
x_{k+1} = (1-\gamma_k)y_{k+1} + \gamma_k y_k
$$

## Continuous-Time Analogous of Acceleration

THe method proposed above can be effectively viewed as the discretization of a second-order ODE,

$$
\ddot{X} + \frac{3}{t} \dot{X} + \nabla f(X) = 0
$$

with initial conditions $X(0) = x_0$, $\dot{X}(0) = 0$. Such an ODE also enjoys inverse quadratic convergence rate.

$$

f(X(t)) - f^\star \le O \left( \frac{\| x_0 - x^\star \|^2_2}{t^2} \right)$$